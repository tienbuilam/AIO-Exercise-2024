{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 59\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'Auto_MPG_data.csv'\n",
    "dataset = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop(columns='MPG').values\n",
    "y = dataset['MPG'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 0.2\n",
    "test_size = 0.125\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = val_size, random_state=random_state)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = StandardScaler()\n",
    "X_train = normalizer.fit_transform(X_train)\n",
    "X_val = normalizer.transform(X_val)\n",
    "X_test = normalizer.transform(X_test)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dims, hidden_dims)\n",
    "        self.linear2 = nn.Linear(hidden_dims, hidden_dims)\n",
    "        self.output = nn.Linear(hidden_dims, output_dims)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "        out = self.output(x)\n",
    "\n",
    "        return out.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dims = X_train.shape[1]\n",
    "output_dims = 1\n",
    "hidden_dims = 64\n",
    "\n",
    "model = MLP(input_dims, hidden_dims, output_dims).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared(y_true, y_pred):\n",
    "    y_true = torch.Tensor(y_true).to(device)\n",
    "    y_pred = torch.Tensor(y_pred).to(device)\n",
    "    mean_true = torch.mean(y_true)\n",
    "    ss_tot = torch.sum((y_true - mean_true) ** 2)\n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1:\tTraining loss: 90.361\tValidation loss: 20.327\n",
      "\n",
      "EPOCH 2:\tTraining loss: 17.722\tValidation loss: 14.955\n",
      "\n",
      "EPOCH 3:\tTraining loss: 20.481\tValidation loss: 10.068\n",
      "\n",
      "EPOCH 4:\tTraining loss: 38.832\tValidation loss: 38.442\n",
      "\n",
      "EPOCH 5:\tTraining loss: 19.056\tValidation loss: 36.808\n",
      "\n",
      "EPOCH 6:\tTraining loss: 23.355\tValidation loss: 43.035\n",
      "\n",
      "EPOCH 7:\tTraining loss: 29.892\tValidation loss: 22.099\n",
      "\n",
      "EPOCH 8:\tTraining loss: 10.058\tValidation loss: 5.395\n",
      "\n",
      "EPOCH 9:\tTraining loss: 15.174\tValidation loss: 15.289\n",
      "\n",
      "EPOCH 10:\tTraining loss: 12.548\tValidation loss: 14.489\n",
      "\n",
      "EPOCH 11:\tTraining loss: 15.171\tValidation loss: 8.579\n",
      "\n",
      "EPOCH 12:\tTraining loss: 11.173\tValidation loss: 20.875\n",
      "\n",
      "EPOCH 13:\tTraining loss: 12.929\tValidation loss: 14.357\n",
      "\n",
      "EPOCH 14:\tTraining loss: 15.647\tValidation loss: 12.957\n",
      "\n",
      "EPOCH 15:\tTraining loss: 15.186\tValidation loss: 5.237\n",
      "\n",
      "EPOCH 16:\tTraining loss: 7.288\tValidation loss: 5.030\n",
      "\n",
      "EPOCH 17:\tTraining loss: 8.615\tValidation loss: 4.650\n",
      "\n",
      "EPOCH 18:\tTraining loss: 6.197\tValidation loss: 5.466\n",
      "\n",
      "EPOCH 19:\tTraining loss: 10.524\tValidation loss: 52.823\n",
      "\n",
      "EPOCH 20:\tTraining loss: 11.340\tValidation loss: 14.829\n",
      "\n",
      "EPOCH 21:\tTraining loss: 9.064\tValidation loss: 8.779\n",
      "\n",
      "EPOCH 22:\tTraining loss: 10.491\tValidation loss: 6.482\n",
      "\n",
      "EPOCH 23:\tTraining loss: 6.330\tValidation loss: 7.042\n",
      "\n",
      "EPOCH 24:\tTraining loss: 10.590\tValidation loss: 4.899\n",
      "\n",
      "EPOCH 25:\tTraining loss: 6.552\tValidation loss: 11.819\n",
      "\n",
      "EPOCH 26:\tTraining loss: 18.368\tValidation loss: 4.768\n",
      "\n",
      "EPOCH 27:\tTraining loss: 8.114\tValidation loss: 6.100\n",
      "\n",
      "EPOCH 28:\tTraining loss: 7.009\tValidation loss: 6.179\n",
      "\n",
      "EPOCH 29:\tTraining loss: 7.247\tValidation loss: 4.578\n",
      "\n",
      "EPOCH 30:\tTraining loss: 6.792\tValidation loss: 6.598\n",
      "\n",
      "EPOCH 31:\tTraining loss: 7.711\tValidation loss: 6.421\n",
      "\n",
      "EPOCH 32:\tTraining loss: 6.059\tValidation loss: 6.497\n",
      "\n",
      "EPOCH 33:\tTraining loss: 6.438\tValidation loss: 6.080\n",
      "\n",
      "EPOCH 34:\tTraining loss: 7.564\tValidation loss: 10.438\n",
      "\n",
      "EPOCH 35:\tTraining loss: 7.391\tValidation loss: 14.427\n",
      "\n",
      "EPOCH 36:\tTraining loss: 7.565\tValidation loss: 5.654\n",
      "\n",
      "EPOCH 37:\tTraining loss: 5.979\tValidation loss: 23.299\n",
      "\n",
      "EPOCH 38:\tTraining loss: 8.953\tValidation loss: 6.086\n",
      "\n",
      "EPOCH 39:\tTraining loss: 7.525\tValidation loss: 30.022\n",
      "\n",
      "EPOCH 40:\tTraining loss: 10.238\tValidation loss: 11.264\n",
      "\n",
      "EPOCH 41:\tTraining loss: 6.609\tValidation loss: 4.892\n",
      "\n",
      "EPOCH 42:\tTraining loss: 8.088\tValidation loss: 7.392\n",
      "\n",
      "EPOCH 43:\tTraining loss: 6.510\tValidation loss: 6.240\n",
      "\n",
      "EPOCH 44:\tTraining loss: 5.335\tValidation loss: 6.466\n",
      "\n",
      "EPOCH 45:\tTraining loss: 6.581\tValidation loss: 9.438\n",
      "\n",
      "EPOCH 46:\tTraining loss: 6.848\tValidation loss: 12.107\n",
      "\n",
      "EPOCH 47:\tTraining loss: 8.249\tValidation loss: 9.197\n",
      "\n",
      "EPOCH 48:\tTraining loss: 10.880\tValidation loss: 5.133\n",
      "\n",
      "EPOCH 49:\tTraining loss: 7.352\tValidation loss: 4.974\n",
      "\n",
      "EPOCH 50:\tTraining loss: 6.539\tValidation loss: 5.466\n",
      "\n",
      "EPOCH 51:\tTraining loss: 5.543\tValidation loss: 5.049\n",
      "\n",
      "EPOCH 52:\tTraining loss: 5.542\tValidation loss: 6.189\n",
      "\n",
      "EPOCH 53:\tTraining loss: 7.814\tValidation loss: 7.367\n",
      "\n",
      "EPOCH 54:\tTraining loss: 7.256\tValidation loss: 20.787\n",
      "\n",
      "EPOCH 55:\tTraining loss: 8.016\tValidation loss: 8.574\n",
      "\n",
      "EPOCH 56:\tTraining loss: 5.974\tValidation loss: 4.781\n",
      "\n",
      "EPOCH 57:\tTraining loss: 5.579\tValidation loss: 8.988\n",
      "\n",
      "EPOCH 58:\tTraining loss: 10.557\tValidation loss: 4.629\n",
      "\n",
      "EPOCH 59:\tTraining loss: 6.577\tValidation loss: 13.180\n",
      "\n",
      "EPOCH 60:\tTraining loss: 6.277\tValidation loss: 6.293\n",
      "\n",
      "EPOCH 61:\tTraining loss: 8.580\tValidation loss: 5.214\n",
      "\n",
      "EPOCH 62:\tTraining loss: 5.271\tValidation loss: 5.170\n",
      "\n",
      "EPOCH 63:\tTraining loss: 5.914\tValidation loss: 5.588\n",
      "\n",
      "EPOCH 64:\tTraining loss: 9.358\tValidation loss: 6.984\n",
      "\n",
      "EPOCH 65:\tTraining loss: 7.142\tValidation loss: 9.026\n",
      "\n",
      "EPOCH 66:\tTraining loss: 6.064\tValidation loss: 5.364\n",
      "\n",
      "EPOCH 67:\tTraining loss: 5.278\tValidation loss: 4.641\n",
      "\n",
      "EPOCH 68:\tTraining loss: 5.842\tValidation loss: 8.755\n",
      "\n",
      "EPOCH 69:\tTraining loss: 6.886\tValidation loss: 10.547\n",
      "\n",
      "EPOCH 70:\tTraining loss: 6.284\tValidation loss: 5.226\n",
      "\n",
      "EPOCH 71:\tTraining loss: 5.669\tValidation loss: 16.061\n",
      "\n",
      "EPOCH 72:\tTraining loss: 7.407\tValidation loss: 17.499\n",
      "\n",
      "EPOCH 73:\tTraining loss: 11.274\tValidation loss: 7.463\n",
      "\n",
      "EPOCH 74:\tTraining loss: 6.964\tValidation loss: 5.360\n",
      "\n",
      "EPOCH 75:\tTraining loss: 6.127\tValidation loss: 11.804\n",
      "\n",
      "EPOCH 76:\tTraining loss: 8.362\tValidation loss: 5.805\n",
      "\n",
      "EPOCH 77:\tTraining loss: 10.287\tValidation loss: 5.608\n",
      "\n",
      "EPOCH 78:\tTraining loss: 5.742\tValidation loss: 5.774\n",
      "\n",
      "EPOCH 79:\tTraining loss: 5.489\tValidation loss: 7.985\n",
      "\n",
      "EPOCH 80:\tTraining loss: 5.406\tValidation loss: 5.124\n",
      "\n",
      "EPOCH 81:\tTraining loss: 5.027\tValidation loss: 6.350\n",
      "\n",
      "EPOCH 82:\tTraining loss: 4.866\tValidation loss: 6.355\n",
      "\n",
      "EPOCH 83:\tTraining loss: 5.136\tValidation loss: 7.040\n",
      "\n",
      "EPOCH 84:\tTraining loss: 5.167\tValidation loss: 5.052\n",
      "\n",
      "EPOCH 85:\tTraining loss: 4.888\tValidation loss: 5.160\n",
      "\n",
      "EPOCH 86:\tTraining loss: 6.985\tValidation loss: 6.397\n",
      "\n",
      "EPOCH 87:\tTraining loss: 5.312\tValidation loss: 6.481\n",
      "\n",
      "EPOCH 88:\tTraining loss: 7.199\tValidation loss: 5.183\n",
      "\n",
      "EPOCH 89:\tTraining loss: 6.037\tValidation loss: 4.996\n",
      "\n",
      "EPOCH 90:\tTraining loss: 9.424\tValidation loss: 7.267\n",
      "\n",
      "EPOCH 91:\tTraining loss: 6.378\tValidation loss: 6.018\n",
      "\n",
      "EPOCH 92:\tTraining loss: 8.638\tValidation loss: 21.091\n",
      "\n",
      "EPOCH 93:\tTraining loss: 7.416\tValidation loss: 10.195\n",
      "\n",
      "EPOCH 94:\tTraining loss: 6.743\tValidation loss: 7.586\n",
      "\n",
      "EPOCH 95:\tTraining loss: 6.287\tValidation loss: 5.354\n",
      "\n",
      "EPOCH 96:\tTraining loss: 6.073\tValidation loss: 4.805\n",
      "\n",
      "EPOCH 97:\tTraining loss: 5.512\tValidation loss: 27.768\n",
      "\n",
      "EPOCH 98:\tTraining loss: 10.957\tValidation loss: 5.035\n",
      "\n",
      "EPOCH 99:\tTraining loss: 5.287\tValidation loss: 4.207\n",
      "\n",
      "EPOCH 100:\tTraining loss: 5.021\tValidation loss: 12.040\n",
      "\n",
      "EPOCH 101:\tTraining loss: 7.549\tValidation loss: 5.098\n",
      "\n",
      "EPOCH 102:\tTraining loss: 5.638\tValidation loss: 4.240\n",
      "\n",
      "EPOCH 103:\tTraining loss: 4.440\tValidation loss: 5.337\n",
      "\n",
      "EPOCH 104:\tTraining loss: 6.678\tValidation loss: 6.534\n",
      "\n",
      "EPOCH 105:\tTraining loss: 6.240\tValidation loss: 5.901\n",
      "\n",
      "EPOCH 106:\tTraining loss: 6.137\tValidation loss: 16.560\n",
      "\n",
      "EPOCH 107:\tTraining loss: 5.907\tValidation loss: 7.616\n",
      "\n",
      "EPOCH 108:\tTraining loss: 4.622\tValidation loss: 5.621\n",
      "\n",
      "EPOCH 109:\tTraining loss: 5.415\tValidation loss: 6.540\n",
      "\n",
      "EPOCH 110:\tTraining loss: 7.648\tValidation loss: 7.484\n",
      "\n",
      "EPOCH 111:\tTraining loss: 8.806\tValidation loss: 7.581\n",
      "\n",
      "EPOCH 112:\tTraining loss: 5.629\tValidation loss: 6.756\n",
      "\n",
      "EPOCH 113:\tTraining loss: 7.691\tValidation loss: 8.420\n",
      "\n",
      "EPOCH 114:\tTraining loss: 5.263\tValidation loss: 6.799\n",
      "\n",
      "EPOCH 115:\tTraining loss: 6.610\tValidation loss: 6.146\n",
      "\n",
      "EPOCH 116:\tTraining loss: 5.607\tValidation loss: 5.718\n",
      "\n",
      "EPOCH 117:\tTraining loss: 4.512\tValidation loss: 4.482\n",
      "\n",
      "EPOCH 118:\tTraining loss: 5.516\tValidation loss: 18.640\n",
      "\n",
      "EPOCH 119:\tTraining loss: 9.191\tValidation loss: 10.529\n",
      "\n",
      "EPOCH 120:\tTraining loss: 6.436\tValidation loss: 8.682\n",
      "\n",
      "EPOCH 121:\tTraining loss: 4.831\tValidation loss: 4.895\n",
      "\n",
      "EPOCH 122:\tTraining loss: 5.045\tValidation loss: 6.390\n",
      "\n",
      "EPOCH 123:\tTraining loss: 5.568\tValidation loss: 7.812\n",
      "\n",
      "EPOCH 124:\tTraining loss: 4.941\tValidation loss: 5.332\n",
      "\n",
      "EPOCH 125:\tTraining loss: 5.768\tValidation loss: 5.882\n",
      "\n",
      "EPOCH 126:\tTraining loss: 6.072\tValidation loss: 6.806\n",
      "\n",
      "EPOCH 127:\tTraining loss: 7.726\tValidation loss: 5.184\n",
      "\n",
      "EPOCH 128:\tTraining loss: 6.218\tValidation loss: 5.504\n",
      "\n",
      "EPOCH 129:\tTraining loss: 5.457\tValidation loss: 7.875\n",
      "\n",
      "EPOCH 130:\tTraining loss: 5.981\tValidation loss: 9.825\n",
      "\n",
      "EPOCH 131:\tTraining loss: 9.755\tValidation loss: 8.810\n",
      "\n",
      "EPOCH 132:\tTraining loss: 4.540\tValidation loss: 5.599\n",
      "\n",
      "EPOCH 133:\tTraining loss: 6.608\tValidation loss: 7.778\n",
      "\n",
      "EPOCH 134:\tTraining loss: 6.614\tValidation loss: 4.924\n",
      "\n",
      "EPOCH 135:\tTraining loss: 7.907\tValidation loss: 11.737\n",
      "\n",
      "EPOCH 136:\tTraining loss: 5.204\tValidation loss: 5.953\n",
      "\n",
      "EPOCH 137:\tTraining loss: 4.549\tValidation loss: 5.156\n",
      "\n",
      "EPOCH 138:\tTraining loss: 5.987\tValidation loss: 5.418\n",
      "\n",
      "EPOCH 139:\tTraining loss: 6.496\tValidation loss: 10.588\n",
      "\n",
      "EPOCH 140:\tTraining loss: 5.533\tValidation loss: 7.932\n",
      "\n",
      "EPOCH 141:\tTraining loss: 10.585\tValidation loss: 8.379\n",
      "\n",
      "EPOCH 142:\tTraining loss: 5.780\tValidation loss: 5.932\n",
      "\n",
      "EPOCH 143:\tTraining loss: 4.962\tValidation loss: 5.704\n",
      "\n",
      "EPOCH 144:\tTraining loss: 4.780\tValidation loss: 5.006\n",
      "\n",
      "EPOCH 145:\tTraining loss: 6.198\tValidation loss: 8.506\n",
      "\n",
      "EPOCH 146:\tTraining loss: 4.611\tValidation loss: 7.155\n",
      "\n",
      "EPOCH 147:\tTraining loss: 5.856\tValidation loss: 5.032\n",
      "\n",
      "EPOCH 148:\tTraining loss: 4.987\tValidation loss: 5.599\n",
      "\n",
      "EPOCH 149:\tTraining loss: 4.443\tValidation loss: 5.484\n",
      "\n",
      "EPOCH 150:\tTraining loss: 6.185\tValidation loss: 6.088\n",
      "\n",
      "EPOCH 151:\tTraining loss: 4.720\tValidation loss: 5.066\n",
      "\n",
      "EPOCH 152:\tTraining loss: 5.920\tValidation loss: 5.131\n",
      "\n",
      "EPOCH 153:\tTraining loss: 7.064\tValidation loss: 4.656\n",
      "\n",
      "EPOCH 154:\tTraining loss: 4.732\tValidation loss: 6.516\n",
      "\n",
      "EPOCH 155:\tTraining loss: 4.557\tValidation loss: 4.250\n",
      "\n",
      "EPOCH 156:\tTraining loss: 5.536\tValidation loss: 5.485\n",
      "\n",
      "EPOCH 157:\tTraining loss: 4.484\tValidation loss: 6.791\n",
      "\n",
      "EPOCH 158:\tTraining loss: 4.581\tValidation loss: 5.543\n",
      "\n",
      "EPOCH 159:\tTraining loss: 4.700\tValidation loss: 4.519\n",
      "\n",
      "EPOCH 160:\tTraining loss: 3.936\tValidation loss: 6.100\n",
      "\n",
      "EPOCH 161:\tTraining loss: 5.749\tValidation loss: 7.791\n",
      "\n",
      "EPOCH 162:\tTraining loss: 5.608\tValidation loss: 5.420\n",
      "\n",
      "EPOCH 163:\tTraining loss: 4.329\tValidation loss: 10.687\n",
      "\n",
      "EPOCH 164:\tTraining loss: 7.393\tValidation loss: 4.958\n",
      "\n",
      "EPOCH 165:\tTraining loss: 5.850\tValidation loss: 15.171\n",
      "\n",
      "EPOCH 166:\tTraining loss: 5.767\tValidation loss: 4.770\n",
      "\n",
      "EPOCH 167:\tTraining loss: 4.070\tValidation loss: 28.608\n",
      "\n",
      "EPOCH 168:\tTraining loss: 6.101\tValidation loss: 5.049\n",
      "\n",
      "EPOCH 169:\tTraining loss: 3.919\tValidation loss: 5.295\n",
      "\n",
      "EPOCH 170:\tTraining loss: 3.854\tValidation loss: 6.428\n",
      "\n",
      "EPOCH 171:\tTraining loss: 5.163\tValidation loss: 9.074\n",
      "\n",
      "EPOCH 172:\tTraining loss: 4.891\tValidation loss: 6.246\n",
      "\n",
      "EPOCH 173:\tTraining loss: 4.303\tValidation loss: 5.091\n",
      "\n",
      "EPOCH 174:\tTraining loss: 4.590\tValidation loss: 6.917\n",
      "\n",
      "EPOCH 175:\tTraining loss: 6.724\tValidation loss: 13.466\n",
      "\n",
      "EPOCH 176:\tTraining loss: 7.634\tValidation loss: 5.631\n",
      "\n",
      "EPOCH 177:\tTraining loss: 3.789\tValidation loss: 4.896\n",
      "\n",
      "EPOCH 178:\tTraining loss: 4.580\tValidation loss: 10.778\n",
      "\n",
      "EPOCH 179:\tTraining loss: 7.211\tValidation loss: 6.296\n",
      "\n",
      "EPOCH 180:\tTraining loss: 5.506\tValidation loss: 6.750\n",
      "\n",
      "EPOCH 181:\tTraining loss: 4.259\tValidation loss: 5.357\n",
      "\n",
      "EPOCH 182:\tTraining loss: 5.476\tValidation loss: 4.753\n",
      "\n",
      "EPOCH 183:\tTraining loss: 4.376\tValidation loss: 7.300\n",
      "\n",
      "EPOCH 184:\tTraining loss: 4.942\tValidation loss: 8.048\n",
      "\n",
      "EPOCH 185:\tTraining loss: 7.800\tValidation loss: 5.450\n",
      "\n",
      "EPOCH 186:\tTraining loss: 4.164\tValidation loss: 4.860\n",
      "\n",
      "EPOCH 187:\tTraining loss: 4.221\tValidation loss: 6.170\n",
      "\n",
      "EPOCH 188:\tTraining loss: 3.515\tValidation loss: 7.617\n",
      "\n",
      "EPOCH 189:\tTraining loss: 4.685\tValidation loss: 6.155\n",
      "\n",
      "EPOCH 190:\tTraining loss: 3.789\tValidation loss: 5.093\n",
      "\n",
      "EPOCH 191:\tTraining loss: 3.981\tValidation loss: 7.419\n",
      "\n",
      "EPOCH 192:\tTraining loss: 5.509\tValidation loss: 8.646\n",
      "\n",
      "EPOCH 193:\tTraining loss: 3.796\tValidation loss: 5.729\n",
      "\n",
      "EPOCH 194:\tTraining loss: 3.874\tValidation loss: 6.157\n",
      "\n",
      "EPOCH 195:\tTraining loss: 5.301\tValidation loss: 5.962\n",
      "\n",
      "EPOCH 196:\tTraining loss: 3.855\tValidation loss: 6.915\n",
      "\n",
      "EPOCH 197:\tTraining loss: 3.626\tValidation loss: 5.393\n",
      "\n",
      "EPOCH 198:\tTraining loss: 5.644\tValidation loss: 9.553\n",
      "\n",
      "EPOCH 199:\tTraining loss: 5.046\tValidation loss: 5.832\n",
      "\n",
      "EPOCH 200:\tTraining loss: 3.902\tValidation loss: 9.246\n",
      "\n",
      "EPOCH 201:\tTraining loss: 3.652\tValidation loss: 5.734\n",
      "\n",
      "EPOCH 202:\tTraining loss: 4.896\tValidation loss: 7.732\n",
      "\n",
      "EPOCH 203:\tTraining loss: 7.258\tValidation loss: 9.267\n",
      "\n",
      "EPOCH 204:\tTraining loss: 6.568\tValidation loss: 5.158\n",
      "\n",
      "EPOCH 205:\tTraining loss: 6.203\tValidation loss: 6.278\n",
      "\n",
      "EPOCH 206:\tTraining loss: 4.780\tValidation loss: 8.751\n",
      "\n",
      "EPOCH 207:\tTraining loss: 4.410\tValidation loss: 5.793\n",
      "\n",
      "EPOCH 208:\tTraining loss: 4.051\tValidation loss: 6.201\n",
      "\n",
      "EPOCH 209:\tTraining loss: 3.651\tValidation loss: 6.137\n",
      "\n",
      "EPOCH 210:\tTraining loss: 3.828\tValidation loss: 4.750\n",
      "\n",
      "EPOCH 211:\tTraining loss: 5.711\tValidation loss: 6.095\n",
      "\n",
      "EPOCH 212:\tTraining loss: 4.455\tValidation loss: 6.506\n",
      "\n",
      "EPOCH 213:\tTraining loss: 4.941\tValidation loss: 5.955\n",
      "\n",
      "EPOCH 214:\tTraining loss: 3.964\tValidation loss: 6.143\n",
      "\n",
      "EPOCH 215:\tTraining loss: 4.040\tValidation loss: 5.658\n",
      "\n",
      "EPOCH 216:\tTraining loss: 3.558\tValidation loss: 5.345\n",
      "\n",
      "EPOCH 217:\tTraining loss: 3.693\tValidation loss: 6.565\n",
      "\n",
      "EPOCH 218:\tTraining loss: 3.650\tValidation loss: 8.866\n",
      "\n",
      "EPOCH 219:\tTraining loss: 3.875\tValidation loss: 9.121\n",
      "\n",
      "EPOCH 220:\tTraining loss: 7.104\tValidation loss: 6.910\n",
      "\n",
      "EPOCH 221:\tTraining loss: 4.210\tValidation loss: 6.571\n",
      "\n",
      "EPOCH 222:\tTraining loss: 3.969\tValidation loss: 5.481\n",
      "\n",
      "EPOCH 223:\tTraining loss: 5.444\tValidation loss: 10.025\n",
      "\n",
      "EPOCH 224:\tTraining loss: 4.353\tValidation loss: 5.347\n",
      "\n",
      "EPOCH 225:\tTraining loss: 4.688\tValidation loss: 10.013\n",
      "\n",
      "EPOCH 226:\tTraining loss: 4.545\tValidation loss: 5.168\n",
      "\n",
      "EPOCH 227:\tTraining loss: 3.509\tValidation loss: 8.556\n",
      "\n",
      "EPOCH 228:\tTraining loss: 3.543\tValidation loss: 5.659\n",
      "\n",
      "EPOCH 229:\tTraining loss: 4.611\tValidation loss: 7.922\n",
      "\n",
      "EPOCH 230:\tTraining loss: 5.036\tValidation loss: 5.989\n",
      "\n",
      "EPOCH 231:\tTraining loss: 5.407\tValidation loss: 7.226\n",
      "\n",
      "EPOCH 232:\tTraining loss: 4.926\tValidation loss: 9.835\n",
      "\n",
      "EPOCH 233:\tTraining loss: 4.903\tValidation loss: 7.206\n",
      "\n",
      "EPOCH 234:\tTraining loss: 3.762\tValidation loss: 5.974\n",
      "\n",
      "EPOCH 235:\tTraining loss: 3.621\tValidation loss: 10.155\n",
      "\n",
      "EPOCH 236:\tTraining loss: 3.883\tValidation loss: 7.206\n",
      "\n",
      "EPOCH 237:\tTraining loss: 3.059\tValidation loss: 6.385\n",
      "\n",
      "EPOCH 238:\tTraining loss: 3.399\tValidation loss: 6.700\n",
      "\n",
      "EPOCH 239:\tTraining loss: 3.529\tValidation loss: 6.047\n",
      "\n",
      "EPOCH 240:\tTraining loss: 3.553\tValidation loss: 5.502\n",
      "\n",
      "EPOCH 241:\tTraining loss: 3.763\tValidation loss: 9.125\n",
      "\n",
      "EPOCH 242:\tTraining loss: 3.938\tValidation loss: 5.821\n",
      "\n",
      "EPOCH 243:\tTraining loss: 3.971\tValidation loss: 7.017\n",
      "\n",
      "EPOCH 244:\tTraining loss: 3.744\tValidation loss: 7.375\n",
      "\n",
      "EPOCH 245:\tTraining loss: 5.310\tValidation loss: 6.752\n",
      "\n",
      "EPOCH 246:\tTraining loss: 4.309\tValidation loss: 5.785\n",
      "\n",
      "EPOCH 247:\tTraining loss: 4.041\tValidation loss: 7.058\n",
      "\n",
      "EPOCH 248:\tTraining loss: 3.303\tValidation loss: 8.160\n",
      "\n",
      "EPOCH 249:\tTraining loss: 3.597\tValidation loss: 6.310\n",
      "\n",
      "EPOCH 250:\tTraining loss: 4.265\tValidation loss: 7.762\n",
      "\n",
      "EPOCH 251:\tTraining loss: 4.009\tValidation loss: 5.546\n",
      "\n",
      "EPOCH 252:\tTraining loss: 4.155\tValidation loss: 19.108\n",
      "\n",
      "EPOCH 253:\tTraining loss: 6.305\tValidation loss: 7.213\n",
      "\n",
      "EPOCH 254:\tTraining loss: 3.199\tValidation loss: 8.916\n",
      "\n",
      "EPOCH 255:\tTraining loss: 3.826\tValidation loss: 13.101\n",
      "\n",
      "EPOCH 256:\tTraining loss: 6.512\tValidation loss: 7.922\n",
      "\n",
      "EPOCH 257:\tTraining loss: 3.182\tValidation loss: 7.006\n",
      "\n",
      "EPOCH 258:\tTraining loss: 6.653\tValidation loss: 6.014\n",
      "\n",
      "EPOCH 259:\tTraining loss: 3.853\tValidation loss: 6.404\n",
      "\n",
      "EPOCH 260:\tTraining loss: 3.607\tValidation loss: 6.270\n",
      "\n",
      "EPOCH 261:\tTraining loss: 3.628\tValidation loss: 6.797\n",
      "\n",
      "EPOCH 262:\tTraining loss: 3.617\tValidation loss: 11.373\n",
      "\n",
      "EPOCH 263:\tTraining loss: 6.702\tValidation loss: 11.922\n",
      "\n",
      "EPOCH 264:\tTraining loss: 8.039\tValidation loss: 7.524\n",
      "\n",
      "EPOCH 265:\tTraining loss: 3.502\tValidation loss: 6.390\n",
      "\n",
      "EPOCH 266:\tTraining loss: 4.178\tValidation loss: 6.576\n",
      "\n",
      "EPOCH 267:\tTraining loss: 3.450\tValidation loss: 5.748\n",
      "\n",
      "EPOCH 268:\tTraining loss: 3.257\tValidation loss: 7.514\n",
      "\n",
      "EPOCH 269:\tTraining loss: 3.498\tValidation loss: 7.496\n",
      "\n",
      "EPOCH 270:\tTraining loss: 3.695\tValidation loss: 7.265\n",
      "\n",
      "EPOCH 271:\tTraining loss: 3.033\tValidation loss: 5.883\n",
      "\n",
      "EPOCH 272:\tTraining loss: 3.628\tValidation loss: 5.673\n",
      "\n",
      "EPOCH 273:\tTraining loss: 3.559\tValidation loss: 5.882\n",
      "\n",
      "EPOCH 274:\tTraining loss: 2.840\tValidation loss: 6.282\n",
      "\n",
      "EPOCH 275:\tTraining loss: 3.060\tValidation loss: 5.905\n",
      "\n",
      "EPOCH 276:\tTraining loss: 4.544\tValidation loss: 6.643\n",
      "\n",
      "EPOCH 277:\tTraining loss: 3.483\tValidation loss: 6.099\n",
      "\n",
      "EPOCH 278:\tTraining loss: 4.497\tValidation loss: 7.801\n",
      "\n",
      "EPOCH 279:\tTraining loss: 3.353\tValidation loss: 5.869\n",
      "\n",
      "EPOCH 280:\tTraining loss: 3.045\tValidation loss: 5.561\n",
      "\n",
      "EPOCH 281:\tTraining loss: 2.971\tValidation loss: 6.482\n",
      "\n",
      "EPOCH 282:\tTraining loss: 3.188\tValidation loss: 14.663\n",
      "\n",
      "EPOCH 283:\tTraining loss: 4.714\tValidation loss: 7.663\n",
      "\n",
      "EPOCH 284:\tTraining loss: 4.766\tValidation loss: 8.621\n",
      "\n",
      "EPOCH 285:\tTraining loss: 5.408\tValidation loss: 6.622\n",
      "\n",
      "EPOCH 286:\tTraining loss: 3.795\tValidation loss: 6.739\n",
      "\n",
      "EPOCH 287:\tTraining loss: 3.266\tValidation loss: 7.283\n",
      "\n",
      "EPOCH 288:\tTraining loss: 4.156\tValidation loss: 7.250\n",
      "\n",
      "EPOCH 289:\tTraining loss: 4.244\tValidation loss: 6.820\n",
      "\n",
      "EPOCH 290:\tTraining loss: 3.186\tValidation loss: 6.168\n",
      "\n",
      "EPOCH 291:\tTraining loss: 4.012\tValidation loss: 5.630\n",
      "\n",
      "EPOCH 292:\tTraining loss: 3.138\tValidation loss: 7.068\n",
      "\n",
      "EPOCH 293:\tTraining loss: 3.279\tValidation loss: 9.993\n",
      "\n",
      "EPOCH 294:\tTraining loss: 2.827\tValidation loss: 6.438\n",
      "\n",
      "EPOCH 295:\tTraining loss: 3.672\tValidation loss: 7.156\n",
      "\n",
      "EPOCH 296:\tTraining loss: 4.411\tValidation loss: 6.421\n",
      "\n",
      "EPOCH 297:\tTraining loss: 3.324\tValidation loss: 6.665\n",
      "\n",
      "EPOCH 298:\tTraining loss: 2.747\tValidation loss: 8.057\n",
      "\n",
      "EPOCH 299:\tTraining loss: 4.468\tValidation loss: 6.521\n",
      "\n",
      "EPOCH 300:\tTraining loss: 4.013\tValidation loss: 6.154\n",
      "\n",
      "EPOCH 301:\tTraining loss: 3.767\tValidation loss: 6.134\n",
      "\n",
      "EPOCH 302:\tTraining loss: 3.712\tValidation loss: 10.596\n",
      "\n",
      "EPOCH 303:\tTraining loss: 4.315\tValidation loss: 8.725\n",
      "\n",
      "EPOCH 304:\tTraining loss: 4.153\tValidation loss: 5.309\n",
      "\n",
      "EPOCH 305:\tTraining loss: 2.988\tValidation loss: 6.601\n",
      "\n",
      "EPOCH 306:\tTraining loss: 2.886\tValidation loss: 6.357\n",
      "\n",
      "EPOCH 307:\tTraining loss: 4.097\tValidation loss: 12.544\n",
      "\n",
      "EPOCH 308:\tTraining loss: 5.116\tValidation loss: 10.159\n",
      "\n",
      "EPOCH 309:\tTraining loss: 3.833\tValidation loss: 6.405\n",
      "\n",
      "EPOCH 310:\tTraining loss: 3.069\tValidation loss: 8.091\n",
      "\n",
      "EPOCH 311:\tTraining loss: 3.213\tValidation loss: 7.811\n",
      "\n",
      "EPOCH 312:\tTraining loss: 3.149\tValidation loss: 6.802\n",
      "\n",
      "EPOCH 313:\tTraining loss: 3.978\tValidation loss: 6.519\n",
      "\n",
      "EPOCH 314:\tTraining loss: 3.129\tValidation loss: 9.583\n",
      "\n",
      "EPOCH 315:\tTraining loss: 4.716\tValidation loss: 6.146\n",
      "\n",
      "EPOCH 316:\tTraining loss: 3.287\tValidation loss: 7.378\n",
      "\n",
      "EPOCH 317:\tTraining loss: 4.022\tValidation loss: 7.471\n",
      "\n",
      "EPOCH 318:\tTraining loss: 3.659\tValidation loss: 6.469\n",
      "\n",
      "EPOCH 319:\tTraining loss: 3.289\tValidation loss: 6.920\n",
      "\n",
      "EPOCH 320:\tTraining loss: 3.164\tValidation loss: 8.811\n",
      "\n",
      "EPOCH 321:\tTraining loss: 3.121\tValidation loss: 6.152\n",
      "\n",
      "EPOCH 322:\tTraining loss: 2.812\tValidation loss: 7.242\n",
      "\n",
      "EPOCH 323:\tTraining loss: 3.386\tValidation loss: 8.364\n",
      "\n",
      "EPOCH 324:\tTraining loss: 3.127\tValidation loss: 6.644\n",
      "\n",
      "EPOCH 325:\tTraining loss: 3.011\tValidation loss: 9.674\n",
      "\n",
      "EPOCH 326:\tTraining loss: 4.252\tValidation loss: 6.775\n",
      "\n",
      "EPOCH 327:\tTraining loss: 5.440\tValidation loss: 12.368\n",
      "\n",
      "EPOCH 328:\tTraining loss: 5.154\tValidation loss: 7.371\n",
      "\n",
      "EPOCH 329:\tTraining loss: 2.541\tValidation loss: 6.789\n",
      "\n",
      "EPOCH 330:\tTraining loss: 2.728\tValidation loss: 6.882\n",
      "\n",
      "EPOCH 331:\tTraining loss: 2.934\tValidation loss: 6.160\n",
      "\n",
      "EPOCH 332:\tTraining loss: 2.830\tValidation loss: 7.408\n",
      "\n",
      "EPOCH 333:\tTraining loss: 3.161\tValidation loss: 6.921\n",
      "\n",
      "EPOCH 334:\tTraining loss: 4.990\tValidation loss: 6.564\n",
      "\n",
      "EPOCH 335:\tTraining loss: 2.715\tValidation loss: 8.558\n",
      "\n",
      "EPOCH 336:\tTraining loss: 2.736\tValidation loss: 8.102\n",
      "\n",
      "EPOCH 337:\tTraining loss: 5.187\tValidation loss: 7.477\n",
      "\n",
      "EPOCH 338:\tTraining loss: 3.218\tValidation loss: 9.427\n",
      "\n",
      "EPOCH 339:\tTraining loss: 3.551\tValidation loss: 8.449\n",
      "\n",
      "EPOCH 340:\tTraining loss: 4.431\tValidation loss: 7.993\n",
      "\n",
      "EPOCH 341:\tTraining loss: 2.669\tValidation loss: 7.351\n",
      "\n",
      "EPOCH 342:\tTraining loss: 2.611\tValidation loss: 6.392\n",
      "\n",
      "EPOCH 343:\tTraining loss: 2.884\tValidation loss: 6.364\n",
      "\n",
      "EPOCH 344:\tTraining loss: 2.763\tValidation loss: 5.690\n",
      "\n",
      "EPOCH 345:\tTraining loss: 3.034\tValidation loss: 6.648\n",
      "\n",
      "EPOCH 346:\tTraining loss: 2.935\tValidation loss: 6.839\n",
      "\n",
      "EPOCH 347:\tTraining loss: 3.075\tValidation loss: 6.393\n",
      "\n",
      "EPOCH 348:\tTraining loss: 3.072\tValidation loss: 22.869\n",
      "\n",
      "EPOCH 349:\tTraining loss: 5.636\tValidation loss: 5.869\n",
      "\n",
      "EPOCH 350:\tTraining loss: 3.523\tValidation loss: 6.391\n",
      "\n",
      "EPOCH 351:\tTraining loss: 3.141\tValidation loss: 8.018\n",
      "\n",
      "EPOCH 352:\tTraining loss: 3.803\tValidation loss: 6.805\n",
      "\n",
      "EPOCH 353:\tTraining loss: 3.016\tValidation loss: 5.614\n",
      "\n",
      "EPOCH 354:\tTraining loss: 2.658\tValidation loss: 7.191\n",
      "\n",
      "EPOCH 355:\tTraining loss: 4.749\tValidation loss: 9.860\n",
      "\n",
      "EPOCH 356:\tTraining loss: 3.468\tValidation loss: 8.396\n",
      "\n",
      "EPOCH 357:\tTraining loss: 3.510\tValidation loss: 7.396\n",
      "\n",
      "EPOCH 358:\tTraining loss: 3.178\tValidation loss: 5.755\n",
      "\n",
      "EPOCH 359:\tTraining loss: 5.082\tValidation loss: 10.441\n",
      "\n",
      "EPOCH 360:\tTraining loss: 5.624\tValidation loss: 10.633\n",
      "\n",
      "EPOCH 361:\tTraining loss: 3.335\tValidation loss: 8.037\n",
      "\n",
      "EPOCH 362:\tTraining loss: 2.911\tValidation loss: 6.316\n",
      "\n",
      "EPOCH 363:\tTraining loss: 3.113\tValidation loss: 7.584\n",
      "\n",
      "EPOCH 364:\tTraining loss: 3.223\tValidation loss: 6.297\n",
      "\n",
      "EPOCH 365:\tTraining loss: 2.820\tValidation loss: 7.260\n",
      "\n",
      "EPOCH 366:\tTraining loss: 3.091\tValidation loss: 7.204\n",
      "\n",
      "EPOCH 367:\tTraining loss: 2.576\tValidation loss: 5.866\n",
      "\n",
      "EPOCH 368:\tTraining loss: 2.729\tValidation loss: 8.789\n",
      "\n",
      "EPOCH 369:\tTraining loss: 2.613\tValidation loss: 7.375\n",
      "\n",
      "EPOCH 370:\tTraining loss: 2.885\tValidation loss: 7.478\n",
      "\n",
      "EPOCH 371:\tTraining loss: 4.090\tValidation loss: 8.942\n",
      "\n",
      "EPOCH 372:\tTraining loss: 2.640\tValidation loss: 9.481\n",
      "\n",
      "EPOCH 373:\tTraining loss: 2.594\tValidation loss: 6.929\n",
      "\n",
      "EPOCH 374:\tTraining loss: 3.292\tValidation loss: 6.676\n",
      "\n",
      "EPOCH 375:\tTraining loss: 5.128\tValidation loss: 6.583\n",
      "\n",
      "EPOCH 376:\tTraining loss: 2.832\tValidation loss: 6.983\n",
      "\n",
      "EPOCH 377:\tTraining loss: 2.613\tValidation loss: 7.560\n",
      "\n",
      "EPOCH 378:\tTraining loss: 3.011\tValidation loss: 10.684\n",
      "\n",
      "EPOCH 379:\tTraining loss: 3.090\tValidation loss: 6.982\n",
      "\n",
      "EPOCH 380:\tTraining loss: 2.675\tValidation loss: 7.177\n",
      "\n",
      "EPOCH 381:\tTraining loss: 2.454\tValidation loss: 6.841\n",
      "\n",
      "EPOCH 382:\tTraining loss: 4.062\tValidation loss: 9.272\n",
      "\n",
      "EPOCH 383:\tTraining loss: 3.456\tValidation loss: 8.300\n",
      "\n",
      "EPOCH 384:\tTraining loss: 2.281\tValidation loss: 6.944\n",
      "\n",
      "EPOCH 385:\tTraining loss: 2.352\tValidation loss: 6.184\n",
      "\n",
      "EPOCH 386:\tTraining loss: 2.511\tValidation loss: 10.812\n",
      "\n",
      "EPOCH 387:\tTraining loss: 3.979\tValidation loss: 6.924\n",
      "\n",
      "EPOCH 388:\tTraining loss: 2.594\tValidation loss: 6.497\n",
      "\n",
      "EPOCH 389:\tTraining loss: 2.594\tValidation loss: 8.022\n",
      "\n",
      "EPOCH 390:\tTraining loss: 4.012\tValidation loss: 8.594\n",
      "\n",
      "EPOCH 391:\tTraining loss: 2.681\tValidation loss: 6.909\n",
      "\n",
      "EPOCH 392:\tTraining loss: 2.866\tValidation loss: 6.284\n",
      "\n",
      "EPOCH 393:\tTraining loss: 2.585\tValidation loss: 10.944\n",
      "\n",
      "EPOCH 394:\tTraining loss: 3.807\tValidation loss: 6.520\n",
      "\n",
      "EPOCH 395:\tTraining loss: 3.322\tValidation loss: 8.482\n",
      "\n",
      "EPOCH 396:\tTraining loss: 2.669\tValidation loss: 6.845\n",
      "\n",
      "EPOCH 397:\tTraining loss: 2.613\tValidation loss: 6.479\n",
      "\n",
      "EPOCH 398:\tTraining loss: 2.816\tValidation loss: 7.988\n",
      "\n",
      "EPOCH 399:\tTraining loss: 2.917\tValidation loss: 7.009\n",
      "\n",
      "EPOCH 400:\tTraining loss: 4.154\tValidation loss: 9.991\n",
      "\n",
      "EPOCH 401:\tTraining loss: 3.499\tValidation loss: 10.802\n",
      "\n",
      "EPOCH 402:\tTraining loss: 3.750\tValidation loss: 7.452\n",
      "\n",
      "EPOCH 403:\tTraining loss: 2.842\tValidation loss: 10.329\n",
      "\n",
      "EPOCH 404:\tTraining loss: 3.246\tValidation loss: 8.049\n",
      "\n",
      "EPOCH 405:\tTraining loss: 2.613\tValidation loss: 8.983\n",
      "\n",
      "EPOCH 406:\tTraining loss: 2.659\tValidation loss: 6.476\n",
      "\n",
      "EPOCH 407:\tTraining loss: 2.503\tValidation loss: 7.249\n",
      "\n",
      "EPOCH 408:\tTraining loss: 2.388\tValidation loss: 8.056\n",
      "\n",
      "EPOCH 409:\tTraining loss: 2.794\tValidation loss: 7.013\n",
      "\n",
      "EPOCH 410:\tTraining loss: 2.606\tValidation loss: 6.649\n",
      "\n",
      "EPOCH 411:\tTraining loss: 2.598\tValidation loss: 8.401\n",
      "\n",
      "EPOCH 412:\tTraining loss: 2.814\tValidation loss: 10.416\n",
      "\n",
      "EPOCH 413:\tTraining loss: 3.302\tValidation loss: 6.907\n",
      "\n",
      "EPOCH 414:\tTraining loss: 2.768\tValidation loss: 6.986\n",
      "\n",
      "EPOCH 415:\tTraining loss: 2.857\tValidation loss: 6.649\n",
      "\n",
      "EPOCH 416:\tTraining loss: 3.441\tValidation loss: 7.900\n",
      "\n",
      "EPOCH 417:\tTraining loss: 3.153\tValidation loss: 10.382\n",
      "\n",
      "EPOCH 418:\tTraining loss: 3.695\tValidation loss: 7.086\n",
      "\n",
      "EPOCH 419:\tTraining loss: 2.692\tValidation loss: 6.614\n",
      "\n",
      "EPOCH 420:\tTraining loss: 2.203\tValidation loss: 7.262\n",
      "\n",
      "EPOCH 421:\tTraining loss: 2.176\tValidation loss: 6.701\n",
      "\n",
      "EPOCH 422:\tTraining loss: 2.770\tValidation loss: 6.417\n",
      "\n",
      "EPOCH 423:\tTraining loss: 3.028\tValidation loss: 9.339\n",
      "\n",
      "EPOCH 424:\tTraining loss: 4.464\tValidation loss: 7.521\n",
      "\n",
      "EPOCH 425:\tTraining loss: 2.496\tValidation loss: 10.126\n",
      "\n",
      "EPOCH 426:\tTraining loss: 2.911\tValidation loss: 6.493\n",
      "\n",
      "EPOCH 427:\tTraining loss: 3.010\tValidation loss: 7.042\n",
      "\n",
      "EPOCH 428:\tTraining loss: 2.819\tValidation loss: 10.330\n",
      "\n",
      "EPOCH 429:\tTraining loss: 4.918\tValidation loss: 7.241\n",
      "\n",
      "EPOCH 430:\tTraining loss: 2.892\tValidation loss: 6.534\n",
      "\n",
      "EPOCH 431:\tTraining loss: 3.048\tValidation loss: 7.355\n",
      "\n",
      "EPOCH 432:\tTraining loss: 2.453\tValidation loss: 6.771\n",
      "\n",
      "EPOCH 433:\tTraining loss: 2.927\tValidation loss: 8.512\n",
      "\n",
      "EPOCH 434:\tTraining loss: 2.657\tValidation loss: 7.066\n",
      "\n",
      "EPOCH 435:\tTraining loss: 2.894\tValidation loss: 7.036\n",
      "\n",
      "EPOCH 436:\tTraining loss: 2.414\tValidation loss: 6.843\n",
      "\n",
      "EPOCH 437:\tTraining loss: 2.568\tValidation loss: 6.943\n",
      "\n",
      "EPOCH 438:\tTraining loss: 2.504\tValidation loss: 7.929\n",
      "\n",
      "EPOCH 439:\tTraining loss: 3.586\tValidation loss: 6.431\n",
      "\n",
      "EPOCH 440:\tTraining loss: 3.833\tValidation loss: 7.441\n",
      "\n",
      "EPOCH 441:\tTraining loss: 2.998\tValidation loss: 12.405\n",
      "\n",
      "EPOCH 442:\tTraining loss: 3.203\tValidation loss: 8.393\n",
      "\n",
      "EPOCH 443:\tTraining loss: 2.706\tValidation loss: 7.810\n",
      "\n",
      "EPOCH 444:\tTraining loss: 3.686\tValidation loss: 7.320\n",
      "\n",
      "EPOCH 445:\tTraining loss: 6.353\tValidation loss: 7.563\n",
      "\n",
      "EPOCH 446:\tTraining loss: 2.564\tValidation loss: 6.234\n",
      "\n",
      "EPOCH 447:\tTraining loss: 2.276\tValidation loss: 6.533\n",
      "\n",
      "EPOCH 448:\tTraining loss: 2.687\tValidation loss: 11.661\n",
      "\n",
      "EPOCH 449:\tTraining loss: 2.874\tValidation loss: 7.029\n",
      "\n",
      "EPOCH 450:\tTraining loss: 2.566\tValidation loss: 7.491\n",
      "\n",
      "EPOCH 451:\tTraining loss: 2.670\tValidation loss: 7.295\n",
      "\n",
      "EPOCH 452:\tTraining loss: 2.920\tValidation loss: 7.094\n",
      "\n",
      "EPOCH 453:\tTraining loss: 2.775\tValidation loss: 15.220\n",
      "\n",
      "EPOCH 454:\tTraining loss: 6.638\tValidation loss: 7.207\n",
      "\n",
      "EPOCH 455:\tTraining loss: 2.353\tValidation loss: 7.857\n",
      "\n",
      "EPOCH 456:\tTraining loss: 2.646\tValidation loss: 7.298\n",
      "\n",
      "EPOCH 457:\tTraining loss: 2.478\tValidation loss: 6.566\n",
      "\n",
      "EPOCH 458:\tTraining loss: 2.996\tValidation loss: 6.393\n",
      "\n",
      "EPOCH 459:\tTraining loss: 2.398\tValidation loss: 7.850\n",
      "\n",
      "EPOCH 460:\tTraining loss: 3.454\tValidation loss: 7.487\n",
      "\n",
      "EPOCH 461:\tTraining loss: 3.179\tValidation loss: 6.662\n",
      "\n",
      "EPOCH 462:\tTraining loss: 2.135\tValidation loss: 6.877\n",
      "\n",
      "EPOCH 463:\tTraining loss: 2.295\tValidation loss: 6.426\n",
      "\n",
      "EPOCH 464:\tTraining loss: 2.824\tValidation loss: 10.687\n",
      "\n",
      "EPOCH 465:\tTraining loss: 3.469\tValidation loss: 7.202\n",
      "\n",
      "EPOCH 466:\tTraining loss: 2.395\tValidation loss: 6.693\n",
      "\n",
      "EPOCH 467:\tTraining loss: 1.978\tValidation loss: 7.268\n",
      "\n",
      "EPOCH 468:\tTraining loss: 2.413\tValidation loss: 7.514\n",
      "\n",
      "EPOCH 469:\tTraining loss: 3.025\tValidation loss: 8.263\n",
      "\n",
      "EPOCH 470:\tTraining loss: 2.860\tValidation loss: 7.227\n",
      "\n",
      "EPOCH 471:\tTraining loss: 2.435\tValidation loss: 7.027\n",
      "\n",
      "EPOCH 472:\tTraining loss: 2.261\tValidation loss: 6.833\n",
      "\n",
      "EPOCH 473:\tTraining loss: 2.325\tValidation loss: 6.966\n",
      "\n",
      "EPOCH 474:\tTraining loss: 2.609\tValidation loss: 7.028\n",
      "\n",
      "EPOCH 475:\tTraining loss: 2.643\tValidation loss: 8.148\n",
      "\n",
      "EPOCH 476:\tTraining loss: 2.443\tValidation loss: 7.050\n",
      "\n",
      "EPOCH 477:\tTraining loss: 2.662\tValidation loss: 8.851\n",
      "\n",
      "EPOCH 478:\tTraining loss: 2.928\tValidation loss: 6.648\n",
      "\n",
      "EPOCH 479:\tTraining loss: 2.583\tValidation loss: 6.882\n",
      "\n",
      "EPOCH 480:\tTraining loss: 2.759\tValidation loss: 6.945\n",
      "\n",
      "EPOCH 481:\tTraining loss: 2.792\tValidation loss: 7.129\n",
      "\n",
      "EPOCH 482:\tTraining loss: 2.414\tValidation loss: 7.037\n",
      "\n",
      "EPOCH 483:\tTraining loss: 2.274\tValidation loss: 7.046\n",
      "\n",
      "EPOCH 484:\tTraining loss: 2.470\tValidation loss: 7.082\n",
      "\n",
      "EPOCH 485:\tTraining loss: 2.920\tValidation loss: 15.319\n",
      "\n",
      "EPOCH 486:\tTraining loss: 6.607\tValidation loss: 6.419\n",
      "\n",
      "EPOCH 487:\tTraining loss: 2.530\tValidation loss: 7.174\n",
      "\n",
      "EPOCH 488:\tTraining loss: 2.294\tValidation loss: 9.122\n",
      "\n",
      "EPOCH 489:\tTraining loss: 3.107\tValidation loss: 8.223\n",
      "\n",
      "EPOCH 490:\tTraining loss: 2.561\tValidation loss: 7.363\n",
      "\n",
      "EPOCH 491:\tTraining loss: 3.315\tValidation loss: 6.142\n",
      "\n",
      "EPOCH 492:\tTraining loss: 2.617\tValidation loss: 7.236\n",
      "\n",
      "EPOCH 493:\tTraining loss: 2.515\tValidation loss: 8.753\n",
      "\n",
      "EPOCH 494:\tTraining loss: 2.852\tValidation loss: 11.154\n",
      "\n",
      "EPOCH 495:\tTraining loss: 3.334\tValidation loss: 7.435\n",
      "\n",
      "EPOCH 496:\tTraining loss: 2.664\tValidation loss: 10.074\n",
      "\n",
      "EPOCH 497:\tTraining loss: 3.540\tValidation loss: 7.774\n",
      "\n",
      "EPOCH 498:\tTraining loss: 2.515\tValidation loss: 6.863\n",
      "\n",
      "EPOCH 499:\tTraining loss: 2.003\tValidation loss: 7.112\n",
      "\n",
      "EPOCH 500:\tTraining loss: 2.325\tValidation loss: 7.128\n",
      "\n",
      "EPOCH 501:\tTraining loss: 2.550\tValidation loss: 7.435\n",
      "\n",
      "EPOCH 502:\tTraining loss: 2.059\tValidation loss: 7.445\n",
      "\n",
      "EPOCH 503:\tTraining loss: 1.792\tValidation loss: 7.403\n",
      "\n",
      "EPOCH 504:\tTraining loss: 1.982\tValidation loss: 7.384\n",
      "\n",
      "EPOCH 505:\tTraining loss: 2.663\tValidation loss: 9.232\n",
      "\n",
      "EPOCH 506:\tTraining loss: 2.599\tValidation loss: 9.165\n",
      "\n",
      "EPOCH 507:\tTraining loss: 2.878\tValidation loss: 9.223\n",
      "\n",
      "EPOCH 508:\tTraining loss: 2.608\tValidation loss: 8.979\n",
      "\n",
      "EPOCH 509:\tTraining loss: 2.843\tValidation loss: 7.447\n",
      "\n",
      "EPOCH 510:\tTraining loss: 2.828\tValidation loss: 9.192\n",
      "\n",
      "EPOCH 511:\tTraining loss: 2.530\tValidation loss: 7.971\n",
      "\n",
      "EPOCH 512:\tTraining loss: 2.304\tValidation loss: 7.980\n",
      "\n",
      "EPOCH 513:\tTraining loss: 2.804\tValidation loss: 7.010\n",
      "\n",
      "EPOCH 514:\tTraining loss: 2.179\tValidation loss: 11.043\n",
      "\n",
      "EPOCH 515:\tTraining loss: 2.596\tValidation loss: 7.766\n",
      "\n",
      "EPOCH 516:\tTraining loss: 2.398\tValidation loss: 7.226\n",
      "\n",
      "EPOCH 517:\tTraining loss: 2.243\tValidation loss: 8.063\n",
      "\n",
      "EPOCH 518:\tTraining loss: 2.064\tValidation loss: 6.840\n",
      "\n",
      "EPOCH 519:\tTraining loss: 3.993\tValidation loss: 6.612\n",
      "\n",
      "EPOCH 520:\tTraining loss: 2.261\tValidation loss: 6.635\n",
      "\n",
      "EPOCH 521:\tTraining loss: 2.480\tValidation loss: 6.973\n",
      "\n",
      "EPOCH 522:\tTraining loss: 2.093\tValidation loss: 6.772\n",
      "\n",
      "EPOCH 523:\tTraining loss: 2.198\tValidation loss: 6.871\n",
      "\n",
      "EPOCH 524:\tTraining loss: 2.037\tValidation loss: 6.838\n",
      "\n",
      "EPOCH 525:\tTraining loss: 1.976\tValidation loss: 7.633\n",
      "\n",
      "EPOCH 526:\tTraining loss: 1.993\tValidation loss: 7.259\n",
      "\n",
      "EPOCH 527:\tTraining loss: 2.008\tValidation loss: 8.310\n",
      "\n",
      "EPOCH 528:\tTraining loss: 2.872\tValidation loss: 9.379\n",
      "\n",
      "EPOCH 529:\tTraining loss: 2.330\tValidation loss: 8.288\n",
      "\n",
      "EPOCH 530:\tTraining loss: 2.859\tValidation loss: 7.349\n",
      "\n",
      "EPOCH 531:\tTraining loss: 2.543\tValidation loss: 8.022\n",
      "\n",
      "EPOCH 532:\tTraining loss: 2.354\tValidation loss: 9.039\n",
      "\n",
      "EPOCH 533:\tTraining loss: 3.770\tValidation loss: 7.235\n",
      "\n",
      "EPOCH 534:\tTraining loss: 2.216\tValidation loss: 7.052\n",
      "\n",
      "EPOCH 535:\tTraining loss: 2.060\tValidation loss: 6.886\n",
      "\n",
      "EPOCH 536:\tTraining loss: 1.910\tValidation loss: 7.250\n",
      "\n",
      "EPOCH 537:\tTraining loss: 3.595\tValidation loss: 20.770\n",
      "\n",
      "EPOCH 538:\tTraining loss: 3.989\tValidation loss: 7.732\n",
      "\n",
      "EPOCH 539:\tTraining loss: 2.511\tValidation loss: 7.620\n",
      "\n",
      "EPOCH 540:\tTraining loss: 1.874\tValidation loss: 7.128\n",
      "\n",
      "EPOCH 541:\tTraining loss: 2.470\tValidation loss: 7.017\n",
      "\n",
      "EPOCH 542:\tTraining loss: 3.186\tValidation loss: 8.219\n",
      "\n",
      "EPOCH 543:\tTraining loss: 1.964\tValidation loss: 7.154\n",
      "\n",
      "EPOCH 544:\tTraining loss: 2.124\tValidation loss: 8.406\n",
      "\n",
      "EPOCH 545:\tTraining loss: 2.155\tValidation loss: 9.569\n",
      "\n",
      "EPOCH 546:\tTraining loss: 3.060\tValidation loss: 9.563\n",
      "\n",
      "EPOCH 547:\tTraining loss: 2.292\tValidation loss: 8.058\n",
      "\n",
      "EPOCH 548:\tTraining loss: 2.653\tValidation loss: 7.530\n",
      "\n",
      "EPOCH 549:\tTraining loss: 2.076\tValidation loss: 7.232\n",
      "\n",
      "EPOCH 550:\tTraining loss: 2.283\tValidation loss: 8.370\n",
      "\n",
      "EPOCH 551:\tTraining loss: 2.534\tValidation loss: 8.383\n",
      "\n",
      "EPOCH 552:\tTraining loss: 2.353\tValidation loss: 7.558\n",
      "\n",
      "EPOCH 553:\tTraining loss: 3.145\tValidation loss: 7.026\n",
      "\n",
      "EPOCH 554:\tTraining loss: 2.154\tValidation loss: 7.544\n",
      "\n",
      "EPOCH 555:\tTraining loss: 2.400\tValidation loss: 7.882\n",
      "\n",
      "EPOCH 556:\tTraining loss: 2.425\tValidation loss: 12.331\n",
      "\n",
      "EPOCH 557:\tTraining loss: 2.206\tValidation loss: 7.008\n",
      "\n",
      "EPOCH 558:\tTraining loss: 1.820\tValidation loss: 7.346\n",
      "\n",
      "EPOCH 559:\tTraining loss: 2.060\tValidation loss: 7.485\n",
      "\n",
      "EPOCH 560:\tTraining loss: 2.467\tValidation loss: 7.315\n",
      "\n",
      "EPOCH 561:\tTraining loss: 2.101\tValidation loss: 7.587\n",
      "\n",
      "EPOCH 562:\tTraining loss: 2.899\tValidation loss: 6.989\n",
      "\n",
      "EPOCH 563:\tTraining loss: 2.241\tValidation loss: 8.752\n",
      "\n",
      "EPOCH 564:\tTraining loss: 2.813\tValidation loss: 7.465\n",
      "\n",
      "EPOCH 565:\tTraining loss: 2.017\tValidation loss: 7.279\n",
      "\n",
      "EPOCH 566:\tTraining loss: 2.314\tValidation loss: 9.413\n",
      "\n",
      "EPOCH 567:\tTraining loss: 2.700\tValidation loss: 7.193\n",
      "\n",
      "EPOCH 568:\tTraining loss: 1.867\tValidation loss: 7.810\n",
      "\n",
      "EPOCH 569:\tTraining loss: 1.907\tValidation loss: 7.774\n",
      "\n",
      "EPOCH 570:\tTraining loss: 2.251\tValidation loss: 7.243\n",
      "\n",
      "EPOCH 571:\tTraining loss: 2.177\tValidation loss: 7.221\n",
      "\n",
      "EPOCH 572:\tTraining loss: 2.306\tValidation loss: 7.474\n",
      "\n",
      "EPOCH 573:\tTraining loss: 3.007\tValidation loss: 6.917\n",
      "\n",
      "EPOCH 574:\tTraining loss: 2.187\tValidation loss: 6.713\n",
      "\n",
      "EPOCH 575:\tTraining loss: 2.149\tValidation loss: 8.660\n",
      "\n",
      "EPOCH 576:\tTraining loss: 2.155\tValidation loss: 7.687\n",
      "\n",
      "EPOCH 577:\tTraining loss: 2.788\tValidation loss: 16.384\n",
      "\n",
      "EPOCH 578:\tTraining loss: 2.636\tValidation loss: 8.445\n",
      "\n",
      "EPOCH 579:\tTraining loss: 2.192\tValidation loss: 6.917\n",
      "\n",
      "EPOCH 580:\tTraining loss: 3.356\tValidation loss: 9.394\n",
      "\n",
      "EPOCH 581:\tTraining loss: 3.235\tValidation loss: 7.105\n",
      "\n",
      "EPOCH 582:\tTraining loss: 2.478\tValidation loss: 7.442\n",
      "\n",
      "EPOCH 583:\tTraining loss: 3.087\tValidation loss: 10.593\n",
      "\n",
      "EPOCH 584:\tTraining loss: 4.434\tValidation loss: 10.530\n",
      "\n",
      "EPOCH 585:\tTraining loss: 2.345\tValidation loss: 7.669\n",
      "\n",
      "EPOCH 586:\tTraining loss: 1.996\tValidation loss: 7.814\n",
      "\n",
      "EPOCH 587:\tTraining loss: 3.177\tValidation loss: 7.063\n",
      "\n",
      "EPOCH 588:\tTraining loss: 3.871\tValidation loss: 9.021\n",
      "\n",
      "EPOCH 589:\tTraining loss: 2.465\tValidation loss: 7.924\n",
      "\n",
      "EPOCH 590:\tTraining loss: 2.003\tValidation loss: 9.237\n",
      "\n",
      "EPOCH 591:\tTraining loss: 2.102\tValidation loss: 9.165\n",
      "\n",
      "EPOCH 592:\tTraining loss: 2.041\tValidation loss: 7.546\n",
      "\n",
      "EPOCH 593:\tTraining loss: 2.132\tValidation loss: 7.861\n",
      "\n",
      "EPOCH 594:\tTraining loss: 2.230\tValidation loss: 7.584\n",
      "\n",
      "EPOCH 595:\tTraining loss: 2.415\tValidation loss: 7.840\n",
      "\n",
      "EPOCH 596:\tTraining loss: 2.058\tValidation loss: 7.858\n",
      "\n",
      "EPOCH 597:\tTraining loss: 1.848\tValidation loss: 8.213\n",
      "\n",
      "EPOCH 598:\tTraining loss: 1.742\tValidation loss: 7.704\n",
      "\n",
      "EPOCH 599:\tTraining loss: 1.745\tValidation loss: 7.213\n",
      "\n",
      "EPOCH 600:\tTraining loss: 1.644\tValidation loss: 8.392\n",
      "\n",
      "EPOCH 601:\tTraining loss: 1.912\tValidation loss: 7.287\n",
      "\n",
      "EPOCH 602:\tTraining loss: 1.907\tValidation loss: 8.123\n",
      "\n",
      "EPOCH 603:\tTraining loss: 2.261\tValidation loss: 9.101\n",
      "\n",
      "EPOCH 604:\tTraining loss: 1.943\tValidation loss: 8.338\n",
      "\n",
      "EPOCH 605:\tTraining loss: 2.087\tValidation loss: 8.090\n",
      "\n",
      "EPOCH 606:\tTraining loss: 2.850\tValidation loss: 8.250\n",
      "\n",
      "EPOCH 607:\tTraining loss: 2.218\tValidation loss: 7.541\n",
      "\n",
      "EPOCH 608:\tTraining loss: 2.085\tValidation loss: 8.238\n",
      "\n",
      "EPOCH 609:\tTraining loss: 2.489\tValidation loss: 7.393\n",
      "\n",
      "EPOCH 610:\tTraining loss: 1.803\tValidation loss: 7.446\n",
      "\n",
      "EPOCH 611:\tTraining loss: 1.820\tValidation loss: 7.189\n",
      "\n",
      "EPOCH 612:\tTraining loss: 1.663\tValidation loss: 8.734\n",
      "\n",
      "EPOCH 613:\tTraining loss: 2.112\tValidation loss: 15.468\n",
      "\n",
      "EPOCH 614:\tTraining loss: 5.356\tValidation loss: 7.387\n",
      "\n",
      "EPOCH 615:\tTraining loss: 2.647\tValidation loss: 6.948\n",
      "\n",
      "EPOCH 616:\tTraining loss: 1.834\tValidation loss: 7.491\n",
      "\n",
      "EPOCH 617:\tTraining loss: 1.937\tValidation loss: 8.333\n",
      "\n",
      "EPOCH 618:\tTraining loss: 2.174\tValidation loss: 7.318\n",
      "\n",
      "EPOCH 619:\tTraining loss: 2.012\tValidation loss: 8.063\n",
      "\n",
      "EPOCH 620:\tTraining loss: 1.959\tValidation loss: 7.974\n",
      "\n",
      "EPOCH 621:\tTraining loss: 1.931\tValidation loss: 7.922\n",
      "\n",
      "EPOCH 622:\tTraining loss: 1.731\tValidation loss: 7.629\n",
      "\n",
      "EPOCH 623:\tTraining loss: 2.660\tValidation loss: 9.312\n",
      "\n",
      "EPOCH 624:\tTraining loss: 2.125\tValidation loss: 7.787\n",
      "\n",
      "EPOCH 625:\tTraining loss: 2.466\tValidation loss: 8.628\n",
      "\n",
      "EPOCH 626:\tTraining loss: 3.154\tValidation loss: 9.523\n",
      "\n",
      "EPOCH 627:\tTraining loss: 4.414\tValidation loss: 8.024\n",
      "\n",
      "EPOCH 628:\tTraining loss: 1.819\tValidation loss: 7.416\n",
      "\n",
      "EPOCH 629:\tTraining loss: 1.869\tValidation loss: 7.058\n",
      "\n",
      "EPOCH 630:\tTraining loss: 1.653\tValidation loss: 7.302\n",
      "\n",
      "EPOCH 631:\tTraining loss: 1.992\tValidation loss: 10.008\n",
      "\n",
      "EPOCH 632:\tTraining loss: 2.769\tValidation loss: 7.213\n",
      "\n",
      "EPOCH 633:\tTraining loss: 2.179\tValidation loss: 11.662\n",
      "\n",
      "EPOCH 634:\tTraining loss: 3.934\tValidation loss: 7.333\n",
      "\n",
      "EPOCH 635:\tTraining loss: 2.879\tValidation loss: 7.932\n",
      "\n",
      "EPOCH 636:\tTraining loss: 2.001\tValidation loss: 11.537\n",
      "\n",
      "EPOCH 637:\tTraining loss: 2.893\tValidation loss: 6.898\n",
      "\n",
      "EPOCH 638:\tTraining loss: 1.686\tValidation loss: 7.683\n",
      "\n",
      "EPOCH 639:\tTraining loss: 2.123\tValidation loss: 11.896\n",
      "\n",
      "EPOCH 640:\tTraining loss: 2.193\tValidation loss: 9.343\n",
      "\n",
      "EPOCH 641:\tTraining loss: 2.496\tValidation loss: 8.057\n",
      "\n",
      "EPOCH 642:\tTraining loss: 1.819\tValidation loss: 7.040\n",
      "\n",
      "EPOCH 643:\tTraining loss: 2.479\tValidation loss: 8.737\n",
      "\n",
      "EPOCH 644:\tTraining loss: 2.318\tValidation loss: 8.543\n",
      "\n",
      "EPOCH 645:\tTraining loss: 2.740\tValidation loss: 8.147\n",
      "\n",
      "EPOCH 646:\tTraining loss: 2.228\tValidation loss: 7.803\n",
      "\n",
      "EPOCH 647:\tTraining loss: 1.613\tValidation loss: 7.416\n",
      "\n",
      "EPOCH 648:\tTraining loss: 1.912\tValidation loss: 7.501\n",
      "\n",
      "EPOCH 649:\tTraining loss: 2.363\tValidation loss: 6.933\n",
      "\n",
      "EPOCH 650:\tTraining loss: 2.135\tValidation loss: 9.959\n",
      "\n",
      "EPOCH 651:\tTraining loss: 1.825\tValidation loss: 7.702\n",
      "\n",
      "EPOCH 652:\tTraining loss: 1.813\tValidation loss: 8.075\n",
      "\n",
      "EPOCH 653:\tTraining loss: 2.295\tValidation loss: 9.206\n",
      "\n",
      "EPOCH 654:\tTraining loss: 1.953\tValidation loss: 7.946\n",
      "\n",
      "EPOCH 655:\tTraining loss: 1.828\tValidation loss: 8.814\n",
      "\n",
      "EPOCH 656:\tTraining loss: 1.729\tValidation loss: 9.065\n",
      "\n",
      "EPOCH 657:\tTraining loss: 2.567\tValidation loss: 7.981\n",
      "\n",
      "EPOCH 658:\tTraining loss: 2.110\tValidation loss: 7.800\n",
      "\n",
      "EPOCH 659:\tTraining loss: 1.822\tValidation loss: 8.504\n",
      "\n",
      "EPOCH 660:\tTraining loss: 1.841\tValidation loss: 7.418\n",
      "\n",
      "EPOCH 661:\tTraining loss: 1.835\tValidation loss: 7.708\n",
      "\n",
      "EPOCH 662:\tTraining loss: 2.122\tValidation loss: 13.115\n",
      "\n",
      "EPOCH 663:\tTraining loss: 3.474\tValidation loss: 7.461\n",
      "\n",
      "EPOCH 664:\tTraining loss: 2.210\tValidation loss: 9.824\n",
      "\n",
      "EPOCH 665:\tTraining loss: 3.554\tValidation loss: 8.250\n",
      "\n",
      "EPOCH 666:\tTraining loss: 2.280\tValidation loss: 8.222\n",
      "\n",
      "EPOCH 667:\tTraining loss: 1.783\tValidation loss: 8.827\n",
      "\n",
      "EPOCH 668:\tTraining loss: 1.994\tValidation loss: 8.793\n",
      "\n",
      "EPOCH 669:\tTraining loss: 1.858\tValidation loss: 11.002\n",
      "\n",
      "EPOCH 670:\tTraining loss: 2.420\tValidation loss: 9.945\n",
      "\n",
      "EPOCH 671:\tTraining loss: 3.112\tValidation loss: 10.261\n",
      "\n",
      "EPOCH 672:\tTraining loss: 2.079\tValidation loss: 7.370\n",
      "\n",
      "EPOCH 673:\tTraining loss: 2.078\tValidation loss: 11.992\n",
      "\n",
      "EPOCH 674:\tTraining loss: 2.398\tValidation loss: 7.074\n",
      "\n",
      "EPOCH 675:\tTraining loss: 2.013\tValidation loss: 7.325\n",
      "\n",
      "EPOCH 676:\tTraining loss: 1.578\tValidation loss: 8.784\n",
      "\n",
      "EPOCH 677:\tTraining loss: 1.764\tValidation loss: 7.548\n",
      "\n",
      "EPOCH 678:\tTraining loss: 1.838\tValidation loss: 8.074\n",
      "\n",
      "EPOCH 679:\tTraining loss: 2.189\tValidation loss: 8.891\n",
      "\n",
      "EPOCH 680:\tTraining loss: 2.116\tValidation loss: 8.282\n",
      "\n",
      "EPOCH 681:\tTraining loss: 1.665\tValidation loss: 7.747\n",
      "\n",
      "EPOCH 682:\tTraining loss: 1.675\tValidation loss: 7.577\n",
      "\n",
      "EPOCH 683:\tTraining loss: 1.738\tValidation loss: 7.177\n",
      "\n",
      "EPOCH 684:\tTraining loss: 2.318\tValidation loss: 12.056\n",
      "\n",
      "EPOCH 685:\tTraining loss: 3.443\tValidation loss: 7.170\n",
      "\n",
      "EPOCH 686:\tTraining loss: 2.089\tValidation loss: 7.761\n",
      "\n",
      "EPOCH 687:\tTraining loss: 1.831\tValidation loss: 7.579\n",
      "\n",
      "EPOCH 688:\tTraining loss: 1.984\tValidation loss: 8.660\n",
      "\n",
      "EPOCH 689:\tTraining loss: 2.137\tValidation loss: 7.560\n",
      "\n",
      "EPOCH 690:\tTraining loss: 2.325\tValidation loss: 7.295\n",
      "\n",
      "EPOCH 691:\tTraining loss: 1.951\tValidation loss: 7.507\n",
      "\n",
      "EPOCH 692:\tTraining loss: 2.146\tValidation loss: 8.270\n",
      "\n",
      "EPOCH 693:\tTraining loss: 1.640\tValidation loss: 7.600\n",
      "\n",
      "EPOCH 694:\tTraining loss: 1.547\tValidation loss: 8.079\n",
      "\n",
      "EPOCH 695:\tTraining loss: 2.035\tValidation loss: 15.702\n",
      "\n",
      "EPOCH 696:\tTraining loss: 2.977\tValidation loss: 9.028\n",
      "\n",
      "EPOCH 697:\tTraining loss: 1.721\tValidation loss: 8.187\n",
      "\n",
      "EPOCH 698:\tTraining loss: 2.313\tValidation loss: 11.141\n",
      "\n",
      "EPOCH 699:\tTraining loss: 2.105\tValidation loss: 8.376\n",
      "\n",
      "EPOCH 700:\tTraining loss: 1.508\tValidation loss: 7.261\n",
      "\n",
      "EPOCH 701:\tTraining loss: 1.552\tValidation loss: 8.072\n",
      "\n",
      "EPOCH 702:\tTraining loss: 1.744\tValidation loss: 7.501\n",
      "\n",
      "EPOCH 703:\tTraining loss: 1.593\tValidation loss: 7.277\n",
      "\n",
      "EPOCH 704:\tTraining loss: 1.798\tValidation loss: 7.755\n",
      "\n",
      "EPOCH 705:\tTraining loss: 1.663\tValidation loss: 7.728\n",
      "\n",
      "EPOCH 706:\tTraining loss: 1.949\tValidation loss: 8.499\n",
      "\n",
      "EPOCH 707:\tTraining loss: 2.506\tValidation loss: 9.165\n",
      "\n",
      "EPOCH 708:\tTraining loss: 2.706\tValidation loss: 8.389\n",
      "\n",
      "EPOCH 709:\tTraining loss: 1.949\tValidation loss: 12.171\n",
      "\n",
      "EPOCH 710:\tTraining loss: 2.414\tValidation loss: 7.697\n",
      "\n",
      "EPOCH 711:\tTraining loss: 1.937\tValidation loss: 8.904\n",
      "\n",
      "EPOCH 712:\tTraining loss: 1.688\tValidation loss: 7.700\n",
      "\n",
      "EPOCH 713:\tTraining loss: 1.582\tValidation loss: 7.903\n",
      "\n",
      "EPOCH 714:\tTraining loss: 1.552\tValidation loss: 7.101\n",
      "\n",
      "EPOCH 715:\tTraining loss: 2.226\tValidation loss: 7.620\n",
      "\n",
      "EPOCH 716:\tTraining loss: 1.696\tValidation loss: 7.614\n",
      "\n",
      "EPOCH 717:\tTraining loss: 1.754\tValidation loss: 8.583\n",
      "\n",
      "EPOCH 718:\tTraining loss: 1.632\tValidation loss: 7.964\n",
      "\n",
      "EPOCH 719:\tTraining loss: 1.501\tValidation loss: 7.546\n",
      "\n",
      "EPOCH 720:\tTraining loss: 2.691\tValidation loss: 8.701\n",
      "\n",
      "EPOCH 721:\tTraining loss: 1.577\tValidation loss: 7.880\n",
      "\n",
      "EPOCH 722:\tTraining loss: 1.422\tValidation loss: 7.553\n",
      "\n",
      "EPOCH 723:\tTraining loss: 1.921\tValidation loss: 7.528\n",
      "\n",
      "EPOCH 724:\tTraining loss: 1.973\tValidation loss: 7.402\n",
      "\n",
      "EPOCH 725:\tTraining loss: 1.680\tValidation loss: 8.122\n",
      "\n",
      "EPOCH 726:\tTraining loss: 1.841\tValidation loss: 7.612\n",
      "\n",
      "EPOCH 727:\tTraining loss: 1.651\tValidation loss: 9.027\n",
      "\n",
      "EPOCH 728:\tTraining loss: 2.103\tValidation loss: 8.397\n",
      "\n",
      "EPOCH 729:\tTraining loss: 1.729\tValidation loss: 7.375\n",
      "\n",
      "EPOCH 730:\tTraining loss: 2.168\tValidation loss: 7.989\n",
      "\n",
      "EPOCH 731:\tTraining loss: 1.633\tValidation loss: 7.391\n",
      "\n",
      "EPOCH 732:\tTraining loss: 1.552\tValidation loss: 8.457\n",
      "\n",
      "EPOCH 733:\tTraining loss: 1.408\tValidation loss: 7.618\n",
      "\n",
      "EPOCH 734:\tTraining loss: 1.424\tValidation loss: 8.166\n",
      "\n",
      "EPOCH 735:\tTraining loss: 2.450\tValidation loss: 8.432\n",
      "\n",
      "EPOCH 736:\tTraining loss: 1.728\tValidation loss: 8.562\n",
      "\n",
      "EPOCH 737:\tTraining loss: 1.974\tValidation loss: 7.444\n",
      "\n",
      "EPOCH 738:\tTraining loss: 1.424\tValidation loss: 7.578\n",
      "\n",
      "EPOCH 739:\tTraining loss: 1.608\tValidation loss: 8.084\n",
      "\n",
      "EPOCH 740:\tTraining loss: 1.430\tValidation loss: 8.779\n",
      "\n",
      "EPOCH 741:\tTraining loss: 1.491\tValidation loss: 7.875\n",
      "\n",
      "EPOCH 742:\tTraining loss: 3.526\tValidation loss: 9.868\n",
      "\n",
      "EPOCH 743:\tTraining loss: 2.197\tValidation loss: 8.750\n",
      "\n",
      "EPOCH 744:\tTraining loss: 2.269\tValidation loss: 8.681\n",
      "\n",
      "EPOCH 745:\tTraining loss: 2.343\tValidation loss: 8.037\n",
      "\n",
      "EPOCH 746:\tTraining loss: 1.510\tValidation loss: 8.318\n",
      "\n",
      "EPOCH 747:\tTraining loss: 1.551\tValidation loss: 7.751\n",
      "\n",
      "EPOCH 748:\tTraining loss: 1.930\tValidation loss: 11.988\n",
      "\n",
      "EPOCH 749:\tTraining loss: 2.205\tValidation loss: 7.856\n",
      "\n",
      "EPOCH 750:\tTraining loss: 1.605\tValidation loss: 7.807\n",
      "\n",
      "EPOCH 751:\tTraining loss: 1.748\tValidation loss: 7.088\n",
      "\n",
      "EPOCH 752:\tTraining loss: 1.544\tValidation loss: 7.323\n",
      "\n",
      "EPOCH 753:\tTraining loss: 1.579\tValidation loss: 7.627\n",
      "\n",
      "EPOCH 754:\tTraining loss: 1.550\tValidation loss: 7.719\n",
      "\n",
      "EPOCH 755:\tTraining loss: 1.881\tValidation loss: 7.385\n",
      "\n",
      "EPOCH 756:\tTraining loss: 1.780\tValidation loss: 7.688\n",
      "\n",
      "EPOCH 757:\tTraining loss: 2.036\tValidation loss: 7.497\n",
      "\n",
      "EPOCH 758:\tTraining loss: 1.288\tValidation loss: 7.938\n",
      "\n",
      "EPOCH 759:\tTraining loss: 1.996\tValidation loss: 10.085\n",
      "\n",
      "EPOCH 760:\tTraining loss: 2.462\tValidation loss: 8.328\n",
      "\n",
      "EPOCH 761:\tTraining loss: 1.613\tValidation loss: 11.422\n",
      "\n",
      "EPOCH 762:\tTraining loss: 2.309\tValidation loss: 8.779\n",
      "\n",
      "EPOCH 763:\tTraining loss: 1.842\tValidation loss: 7.968\n",
      "\n",
      "EPOCH 764:\tTraining loss: 2.080\tValidation loss: 8.102\n",
      "\n",
      "EPOCH 765:\tTraining loss: 1.560\tValidation loss: 8.038\n",
      "\n",
      "EPOCH 766:\tTraining loss: 1.850\tValidation loss: 8.914\n",
      "\n",
      "EPOCH 767:\tTraining loss: 1.943\tValidation loss: 8.036\n",
      "\n",
      "EPOCH 768:\tTraining loss: 2.223\tValidation loss: 7.828\n",
      "\n",
      "EPOCH 769:\tTraining loss: 1.627\tValidation loss: 8.623\n",
      "\n",
      "EPOCH 770:\tTraining loss: 1.541\tValidation loss: 7.579\n",
      "\n",
      "EPOCH 771:\tTraining loss: 1.328\tValidation loss: 7.465\n",
      "\n",
      "EPOCH 772:\tTraining loss: 1.427\tValidation loss: 8.307\n",
      "\n",
      "EPOCH 773:\tTraining loss: 1.605\tValidation loss: 7.632\n",
      "\n",
      "EPOCH 774:\tTraining loss: 1.456\tValidation loss: 7.958\n",
      "\n",
      "EPOCH 775:\tTraining loss: 1.378\tValidation loss: 7.545\n",
      "\n",
      "EPOCH 776:\tTraining loss: 2.653\tValidation loss: 11.998\n",
      "\n",
      "EPOCH 777:\tTraining loss: 2.776\tValidation loss: 8.476\n",
      "\n",
      "EPOCH 778:\tTraining loss: 1.603\tValidation loss: 7.968\n",
      "\n",
      "EPOCH 779:\tTraining loss: 1.639\tValidation loss: 9.993\n",
      "\n",
      "EPOCH 780:\tTraining loss: 2.327\tValidation loss: 8.207\n",
      "\n",
      "EPOCH 781:\tTraining loss: 1.737\tValidation loss: 8.071\n",
      "\n",
      "EPOCH 782:\tTraining loss: 1.705\tValidation loss: 8.710\n",
      "\n",
      "EPOCH 783:\tTraining loss: 1.587\tValidation loss: 7.599\n",
      "\n",
      "EPOCH 784:\tTraining loss: 1.729\tValidation loss: 7.770\n",
      "\n",
      "EPOCH 785:\tTraining loss: 1.428\tValidation loss: 8.309\n",
      "\n",
      "EPOCH 786:\tTraining loss: 1.438\tValidation loss: 8.406\n",
      "\n",
      "EPOCH 787:\tTraining loss: 2.476\tValidation loss: 8.718\n",
      "\n",
      "EPOCH 788:\tTraining loss: 1.483\tValidation loss: 9.217\n",
      "\n",
      "EPOCH 789:\tTraining loss: 1.985\tValidation loss: 7.880\n",
      "\n",
      "EPOCH 790:\tTraining loss: 1.645\tValidation loss: 7.997\n",
      "\n",
      "EPOCH 791:\tTraining loss: 3.643\tValidation loss: 8.755\n",
      "\n",
      "EPOCH 792:\tTraining loss: 1.549\tValidation loss: 8.323\n",
      "\n",
      "EPOCH 793:\tTraining loss: 1.703\tValidation loss: 8.872\n",
      "\n",
      "EPOCH 794:\tTraining loss: 1.756\tValidation loss: 8.300\n",
      "\n",
      "EPOCH 795:\tTraining loss: 1.532\tValidation loss: 8.074\n",
      "\n",
      "EPOCH 796:\tTraining loss: 2.095\tValidation loss: 10.910\n",
      "\n",
      "EPOCH 797:\tTraining loss: 4.057\tValidation loss: 9.368\n",
      "\n",
      "EPOCH 798:\tTraining loss: 1.824\tValidation loss: 7.891\n",
      "\n",
      "EPOCH 799:\tTraining loss: 1.898\tValidation loss: 8.168\n",
      "\n",
      "EPOCH 800:\tTraining loss: 2.232\tValidation loss: 7.279\n",
      "\n",
      "EPOCH 801:\tTraining loss: 1.907\tValidation loss: 9.516\n",
      "\n",
      "EPOCH 802:\tTraining loss: 1.854\tValidation loss: 8.932\n",
      "\n",
      "EPOCH 803:\tTraining loss: 1.461\tValidation loss: 7.559\n",
      "\n",
      "EPOCH 804:\tTraining loss: 1.479\tValidation loss: 7.749\n",
      "\n",
      "EPOCH 805:\tTraining loss: 2.034\tValidation loss: 9.743\n",
      "\n",
      "EPOCH 806:\tTraining loss: 1.906\tValidation loss: 7.637\n",
      "\n",
      "EPOCH 807:\tTraining loss: 1.810\tValidation loss: 8.710\n",
      "\n",
      "EPOCH 808:\tTraining loss: 2.633\tValidation loss: 9.604\n",
      "\n",
      "EPOCH 809:\tTraining loss: 1.754\tValidation loss: 8.481\n",
      "\n",
      "EPOCH 810:\tTraining loss: 2.686\tValidation loss: 7.921\n",
      "\n",
      "EPOCH 811:\tTraining loss: 2.010\tValidation loss: 9.183\n",
      "\n",
      "EPOCH 812:\tTraining loss: 2.471\tValidation loss: 12.274\n",
      "\n",
      "EPOCH 813:\tTraining loss: 3.078\tValidation loss: 7.712\n",
      "\n",
      "EPOCH 814:\tTraining loss: 1.573\tValidation loss: 8.305\n",
      "\n",
      "EPOCH 815:\tTraining loss: 1.508\tValidation loss: 7.919\n",
      "\n",
      "EPOCH 816:\tTraining loss: 2.377\tValidation loss: 7.854\n",
      "\n",
      "EPOCH 817:\tTraining loss: 1.371\tValidation loss: 8.363\n",
      "\n",
      "EPOCH 818:\tTraining loss: 1.559\tValidation loss: 7.545\n",
      "\n",
      "EPOCH 819:\tTraining loss: 1.455\tValidation loss: 8.552\n",
      "\n",
      "EPOCH 820:\tTraining loss: 1.912\tValidation loss: 15.452\n",
      "\n",
      "EPOCH 821:\tTraining loss: 2.025\tValidation loss: 9.870\n",
      "\n",
      "EPOCH 822:\tTraining loss: 2.052\tValidation loss: 7.824\n",
      "\n",
      "EPOCH 823:\tTraining loss: 1.454\tValidation loss: 8.020\n",
      "\n",
      "EPOCH 824:\tTraining loss: 1.435\tValidation loss: 8.532\n",
      "\n",
      "EPOCH 825:\tTraining loss: 1.533\tValidation loss: 7.874\n",
      "\n",
      "EPOCH 826:\tTraining loss: 2.390\tValidation loss: 9.454\n",
      "\n",
      "EPOCH 827:\tTraining loss: 1.687\tValidation loss: 7.383\n",
      "\n",
      "EPOCH 828:\tTraining loss: 2.360\tValidation loss: 8.678\n",
      "\n",
      "EPOCH 829:\tTraining loss: 2.195\tValidation loss: 7.959\n",
      "\n",
      "EPOCH 830:\tTraining loss: 1.604\tValidation loss: 10.167\n",
      "\n",
      "EPOCH 831:\tTraining loss: 2.025\tValidation loss: 8.735\n",
      "\n",
      "EPOCH 832:\tTraining loss: 1.751\tValidation loss: 13.889\n",
      "\n",
      "EPOCH 833:\tTraining loss: 2.237\tValidation loss: 9.898\n",
      "\n",
      "EPOCH 834:\tTraining loss: 2.287\tValidation loss: 7.832\n",
      "\n",
      "EPOCH 835:\tTraining loss: 1.649\tValidation loss: 7.844\n",
      "\n",
      "EPOCH 836:\tTraining loss: 1.358\tValidation loss: 7.942\n",
      "\n",
      "EPOCH 837:\tTraining loss: 1.513\tValidation loss: 9.377\n",
      "\n",
      "EPOCH 838:\tTraining loss: 1.381\tValidation loss: 7.383\n",
      "\n",
      "EPOCH 839:\tTraining loss: 1.488\tValidation loss: 8.232\n",
      "\n",
      "EPOCH 840:\tTraining loss: 1.993\tValidation loss: 7.547\n",
      "\n",
      "EPOCH 841:\tTraining loss: 1.504\tValidation loss: 8.126\n",
      "\n",
      "EPOCH 842:\tTraining loss: 1.442\tValidation loss: 8.222\n",
      "\n",
      "EPOCH 843:\tTraining loss: 2.365\tValidation loss: 7.967\n",
      "\n",
      "EPOCH 844:\tTraining loss: 1.509\tValidation loss: 7.621\n",
      "\n",
      "EPOCH 845:\tTraining loss: 1.341\tValidation loss: 8.246\n",
      "\n",
      "EPOCH 846:\tTraining loss: 2.086\tValidation loss: 8.341\n",
      "\n",
      "EPOCH 847:\tTraining loss: 1.426\tValidation loss: 8.269\n",
      "\n",
      "EPOCH 848:\tTraining loss: 1.916\tValidation loss: 9.768\n",
      "\n",
      "EPOCH 849:\tTraining loss: 1.530\tValidation loss: 8.091\n",
      "\n",
      "EPOCH 850:\tTraining loss: 2.016\tValidation loss: 8.550\n",
      "\n",
      "EPOCH 851:\tTraining loss: 1.428\tValidation loss: 8.104\n",
      "\n",
      "EPOCH 852:\tTraining loss: 1.579\tValidation loss: 12.710\n",
      "\n",
      "EPOCH 853:\tTraining loss: 5.412\tValidation loss: 8.209\n",
      "\n",
      "EPOCH 854:\tTraining loss: 1.642\tValidation loss: 8.426\n",
      "\n",
      "EPOCH 855:\tTraining loss: 1.731\tValidation loss: 9.011\n",
      "\n",
      "EPOCH 856:\tTraining loss: 1.630\tValidation loss: 8.061\n",
      "\n",
      "EPOCH 857:\tTraining loss: 1.425\tValidation loss: 8.236\n",
      "\n",
      "EPOCH 858:\tTraining loss: 1.353\tValidation loss: 8.255\n",
      "\n",
      "EPOCH 859:\tTraining loss: 1.365\tValidation loss: 8.709\n",
      "\n",
      "EPOCH 860:\tTraining loss: 2.713\tValidation loss: 8.348\n",
      "\n",
      "EPOCH 861:\tTraining loss: 1.888\tValidation loss: 8.346\n",
      "\n",
      "EPOCH 862:\tTraining loss: 1.317\tValidation loss: 9.377\n",
      "\n",
      "EPOCH 863:\tTraining loss: 1.507\tValidation loss: 7.903\n",
      "\n",
      "EPOCH 864:\tTraining loss: 1.360\tValidation loss: 10.148\n",
      "\n",
      "EPOCH 865:\tTraining loss: 1.286\tValidation loss: 8.345\n",
      "\n",
      "EPOCH 866:\tTraining loss: 1.294\tValidation loss: 7.540\n",
      "\n",
      "EPOCH 867:\tTraining loss: 2.131\tValidation loss: 9.053\n",
      "\n",
      "EPOCH 868:\tTraining loss: 2.282\tValidation loss: 8.879\n",
      "\n",
      "EPOCH 869:\tTraining loss: 1.920\tValidation loss: 8.314\n",
      "\n",
      "EPOCH 870:\tTraining loss: 1.697\tValidation loss: 7.743\n",
      "\n",
      "EPOCH 871:\tTraining loss: 1.508\tValidation loss: 8.786\n",
      "\n",
      "EPOCH 872:\tTraining loss: 1.477\tValidation loss: 8.742\n",
      "\n",
      "EPOCH 873:\tTraining loss: 2.003\tValidation loss: 7.781\n",
      "\n",
      "EPOCH 874:\tTraining loss: 1.390\tValidation loss: 8.112\n",
      "\n",
      "EPOCH 875:\tTraining loss: 1.880\tValidation loss: 11.579\n",
      "\n",
      "EPOCH 876:\tTraining loss: 2.629\tValidation loss: 8.362\n",
      "\n",
      "EPOCH 877:\tTraining loss: 2.045\tValidation loss: 10.338\n",
      "\n",
      "EPOCH 878:\tTraining loss: 1.704\tValidation loss: 9.248\n",
      "\n",
      "EPOCH 879:\tTraining loss: 1.982\tValidation loss: 9.708\n",
      "\n",
      "EPOCH 880:\tTraining loss: 1.710\tValidation loss: 9.344\n",
      "\n",
      "EPOCH 881:\tTraining loss: 1.903\tValidation loss: 8.105\n",
      "\n",
      "EPOCH 882:\tTraining loss: 1.379\tValidation loss: 7.773\n",
      "\n",
      "EPOCH 883:\tTraining loss: 1.552\tValidation loss: 8.097\n",
      "\n",
      "EPOCH 884:\tTraining loss: 1.856\tValidation loss: 7.603\n",
      "\n",
      "EPOCH 885:\tTraining loss: 1.366\tValidation loss: 8.567\n",
      "\n",
      "EPOCH 886:\tTraining loss: 1.422\tValidation loss: 8.230\n",
      "\n",
      "EPOCH 887:\tTraining loss: 1.288\tValidation loss: 7.924\n",
      "\n",
      "EPOCH 888:\tTraining loss: 1.247\tValidation loss: 8.318\n",
      "\n",
      "EPOCH 889:\tTraining loss: 1.705\tValidation loss: 9.013\n",
      "\n",
      "EPOCH 890:\tTraining loss: 1.921\tValidation loss: 9.428\n",
      "\n",
      "EPOCH 891:\tTraining loss: 1.776\tValidation loss: 8.556\n",
      "\n",
      "EPOCH 892:\tTraining loss: 2.143\tValidation loss: 8.696\n",
      "\n",
      "EPOCH 893:\tTraining loss: 1.315\tValidation loss: 8.476\n",
      "\n",
      "EPOCH 894:\tTraining loss: 1.627\tValidation loss: 10.199\n",
      "\n",
      "EPOCH 895:\tTraining loss: 2.398\tValidation loss: 7.946\n",
      "\n",
      "EPOCH 896:\tTraining loss: 1.570\tValidation loss: 9.225\n",
      "\n",
      "EPOCH 897:\tTraining loss: 1.257\tValidation loss: 7.899\n",
      "\n",
      "EPOCH 898:\tTraining loss: 1.565\tValidation loss: 8.170\n",
      "\n",
      "EPOCH 899:\tTraining loss: 1.676\tValidation loss: 12.444\n",
      "\n",
      "EPOCH 900:\tTraining loss: 4.579\tValidation loss: 8.497\n",
      "\n",
      "EPOCH 901:\tTraining loss: 1.907\tValidation loss: 9.202\n",
      "\n",
      "EPOCH 902:\tTraining loss: 1.779\tValidation loss: 7.839\n",
      "\n",
      "EPOCH 903:\tTraining loss: 1.589\tValidation loss: 9.841\n",
      "\n",
      "EPOCH 904:\tTraining loss: 2.406\tValidation loss: 7.814\n",
      "\n",
      "EPOCH 905:\tTraining loss: 1.962\tValidation loss: 9.214\n",
      "\n",
      "EPOCH 906:\tTraining loss: 1.557\tValidation loss: 8.167\n",
      "\n",
      "EPOCH 907:\tTraining loss: 1.413\tValidation loss: 8.457\n",
      "\n",
      "EPOCH 908:\tTraining loss: 1.696\tValidation loss: 8.342\n",
      "\n",
      "EPOCH 909:\tTraining loss: 1.193\tValidation loss: 8.227\n",
      "\n",
      "EPOCH 910:\tTraining loss: 1.227\tValidation loss: 8.450\n",
      "\n",
      "EPOCH 911:\tTraining loss: 1.328\tValidation loss: 7.715\n",
      "\n",
      "EPOCH 912:\tTraining loss: 1.382\tValidation loss: 8.687\n",
      "\n",
      "EPOCH 913:\tTraining loss: 1.209\tValidation loss: 7.729\n",
      "\n",
      "EPOCH 914:\tTraining loss: 1.275\tValidation loss: 10.038\n",
      "\n",
      "EPOCH 915:\tTraining loss: 1.654\tValidation loss: 8.079\n",
      "\n",
      "EPOCH 916:\tTraining loss: 1.198\tValidation loss: 8.579\n",
      "\n",
      "EPOCH 917:\tTraining loss: 1.567\tValidation loss: 8.616\n",
      "\n",
      "EPOCH 918:\tTraining loss: 1.370\tValidation loss: 8.659\n",
      "\n",
      "EPOCH 919:\tTraining loss: 1.407\tValidation loss: 8.157\n",
      "\n",
      "EPOCH 920:\tTraining loss: 1.281\tValidation loss: 7.822\n",
      "\n",
      "EPOCH 921:\tTraining loss: 1.200\tValidation loss: 8.134\n",
      "\n",
      "EPOCH 922:\tTraining loss: 3.496\tValidation loss: 7.163\n",
      "\n",
      "EPOCH 923:\tTraining loss: 1.415\tValidation loss: 7.847\n",
      "\n",
      "EPOCH 924:\tTraining loss: 1.277\tValidation loss: 9.895\n",
      "\n",
      "EPOCH 925:\tTraining loss: 1.637\tValidation loss: 7.914\n",
      "\n",
      "EPOCH 926:\tTraining loss: 1.190\tValidation loss: 9.048\n",
      "\n",
      "EPOCH 927:\tTraining loss: 1.498\tValidation loss: 8.565\n",
      "\n",
      "EPOCH 928:\tTraining loss: 1.454\tValidation loss: 9.010\n",
      "\n",
      "EPOCH 929:\tTraining loss: 1.080\tValidation loss: 8.348\n",
      "\n",
      "EPOCH 930:\tTraining loss: 1.311\tValidation loss: 8.778\n",
      "\n",
      "EPOCH 931:\tTraining loss: 1.230\tValidation loss: 8.055\n",
      "\n",
      "EPOCH 932:\tTraining loss: 1.400\tValidation loss: 8.602\n",
      "\n",
      "EPOCH 933:\tTraining loss: 1.184\tValidation loss: 8.481\n",
      "\n",
      "EPOCH 934:\tTraining loss: 1.222\tValidation loss: 10.195\n",
      "\n",
      "EPOCH 935:\tTraining loss: 1.768\tValidation loss: 8.190\n",
      "\n",
      "EPOCH 936:\tTraining loss: 1.398\tValidation loss: 9.518\n",
      "\n",
      "EPOCH 937:\tTraining loss: 1.742\tValidation loss: 8.058\n",
      "\n",
      "EPOCH 938:\tTraining loss: 1.271\tValidation loss: 8.234\n",
      "\n",
      "EPOCH 939:\tTraining loss: 1.251\tValidation loss: 8.127\n",
      "\n",
      "EPOCH 940:\tTraining loss: 2.134\tValidation loss: 9.504\n",
      "\n",
      "EPOCH 941:\tTraining loss: 1.780\tValidation loss: 8.371\n",
      "\n",
      "EPOCH 942:\tTraining loss: 1.404\tValidation loss: 8.394\n",
      "\n",
      "EPOCH 943:\tTraining loss: 1.326\tValidation loss: 7.970\n",
      "\n",
      "EPOCH 944:\tTraining loss: 1.889\tValidation loss: 11.501\n",
      "\n",
      "EPOCH 945:\tTraining loss: 2.033\tValidation loss: 8.417\n",
      "\n",
      "EPOCH 946:\tTraining loss: 1.835\tValidation loss: 8.994\n",
      "\n",
      "EPOCH 947:\tTraining loss: 1.792\tValidation loss: 9.448\n",
      "\n",
      "EPOCH 948:\tTraining loss: 1.421\tValidation loss: 8.539\n",
      "\n",
      "EPOCH 949:\tTraining loss: 1.017\tValidation loss: 7.900\n",
      "\n",
      "EPOCH 950:\tTraining loss: 1.150\tValidation loss: 9.644\n",
      "\n",
      "EPOCH 951:\tTraining loss: 1.304\tValidation loss: 8.577\n",
      "\n",
      "EPOCH 952:\tTraining loss: 1.733\tValidation loss: 9.799\n",
      "\n",
      "EPOCH 953:\tTraining loss: 1.431\tValidation loss: 8.427\n",
      "\n",
      "EPOCH 954:\tTraining loss: 1.309\tValidation loss: 8.359\n",
      "\n",
      "EPOCH 955:\tTraining loss: 1.521\tValidation loss: 12.581\n",
      "\n",
      "EPOCH 956:\tTraining loss: 2.647\tValidation loss: 9.097\n",
      "\n",
      "EPOCH 957:\tTraining loss: 1.850\tValidation loss: 10.350\n",
      "\n",
      "EPOCH 958:\tTraining loss: 2.510\tValidation loss: 9.452\n",
      "\n",
      "EPOCH 959:\tTraining loss: 1.377\tValidation loss: 7.955\n",
      "\n",
      "EPOCH 960:\tTraining loss: 1.235\tValidation loss: 9.004\n",
      "\n",
      "EPOCH 961:\tTraining loss: 1.303\tValidation loss: 8.075\n",
      "\n",
      "EPOCH 962:\tTraining loss: 1.167\tValidation loss: 9.660\n",
      "\n",
      "EPOCH 963:\tTraining loss: 1.403\tValidation loss: 9.642\n",
      "\n",
      "EPOCH 964:\tTraining loss: 1.307\tValidation loss: 8.054\n",
      "\n",
      "EPOCH 965:\tTraining loss: 1.712\tValidation loss: 8.796\n",
      "\n",
      "EPOCH 966:\tTraining loss: 1.701\tValidation loss: 9.364\n",
      "\n",
      "EPOCH 967:\tTraining loss: 1.465\tValidation loss: 10.182\n",
      "\n",
      "EPOCH 968:\tTraining loss: 3.025\tValidation loss: 10.532\n",
      "\n",
      "EPOCH 969:\tTraining loss: 1.732\tValidation loss: 8.358\n",
      "\n",
      "EPOCH 970:\tTraining loss: 1.287\tValidation loss: 8.634\n",
      "\n",
      "EPOCH 971:\tTraining loss: 1.375\tValidation loss: 8.576\n",
      "\n",
      "EPOCH 972:\tTraining loss: 1.320\tValidation loss: 9.616\n",
      "\n",
      "EPOCH 973:\tTraining loss: 1.662\tValidation loss: 8.757\n",
      "\n",
      "EPOCH 974:\tTraining loss: 1.129\tValidation loss: 8.021\n",
      "\n",
      "EPOCH 975:\tTraining loss: 1.471\tValidation loss: 13.788\n",
      "\n",
      "EPOCH 976:\tTraining loss: 3.367\tValidation loss: 7.577\n",
      "\n",
      "EPOCH 977:\tTraining loss: 1.216\tValidation loss: 8.596\n",
      "\n",
      "EPOCH 978:\tTraining loss: 1.085\tValidation loss: 7.957\n",
      "\n",
      "EPOCH 979:\tTraining loss: 1.582\tValidation loss: 12.410\n",
      "\n",
      "EPOCH 980:\tTraining loss: 1.877\tValidation loss: 13.973\n",
      "\n",
      "EPOCH 981:\tTraining loss: 2.485\tValidation loss: 9.924\n",
      "\n",
      "EPOCH 982:\tTraining loss: 1.282\tValidation loss: 9.269\n",
      "\n",
      "EPOCH 983:\tTraining loss: 1.257\tValidation loss: 8.816\n",
      "\n",
      "EPOCH 984:\tTraining loss: 1.244\tValidation loss: 8.620\n",
      "\n",
      "EPOCH 985:\tTraining loss: 1.274\tValidation loss: 8.436\n",
      "\n",
      "EPOCH 986:\tTraining loss: 1.265\tValidation loss: 8.462\n",
      "\n",
      "EPOCH 987:\tTraining loss: 1.205\tValidation loss: 8.778\n",
      "\n",
      "EPOCH 988:\tTraining loss: 1.189\tValidation loss: 8.376\n",
      "\n",
      "EPOCH 989:\tTraining loss: 1.336\tValidation loss: 9.689\n",
      "\n",
      "EPOCH 990:\tTraining loss: 1.340\tValidation loss: 8.304\n",
      "\n",
      "EPOCH 991:\tTraining loss: 1.204\tValidation loss: 8.407\n",
      "\n",
      "EPOCH 992:\tTraining loss: 1.456\tValidation loss: 8.737\n",
      "\n",
      "EPOCH 993:\tTraining loss: 1.218\tValidation loss: 8.097\n",
      "\n",
      "EPOCH 994:\tTraining loss: 1.303\tValidation loss: 8.646\n",
      "\n",
      "EPOCH 995:\tTraining loss: 1.669\tValidation loss: 9.916\n",
      "\n",
      "EPOCH 996:\tTraining loss: 1.458\tValidation loss: 10.506\n",
      "\n",
      "EPOCH 997:\tTraining loss: 1.238\tValidation loss: 9.442\n",
      "\n",
      "EPOCH 998:\tTraining loss: 1.672\tValidation loss: 8.801\n",
      "\n",
      "EPOCH 999:\tTraining loss: 1.310\tValidation loss: 8.241\n",
      "\n",
      "EPOCH 1000:\tTraining loss: 1.660\tValidation loss: 9.490\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_r2 = []\n",
    "val_r2 = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    train_target = []\n",
    "    val_target = []\n",
    "    train_predict = []\n",
    "    val_predict = []\n",
    "    model.train()\n",
    "\n",
    "    for X_samples, y_samples in train_loader:\n",
    "        X_samples = X_samples.to(device)\n",
    "        y_samples = y_samples.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_samples)\n",
    "        train_predict += outputs.tolist()\n",
    "        train_target += y_samples.tolist()\n",
    "        loss = criterion(outputs, y_samples)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_r2.append(r_squared(train_target, train_predict))\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_samples, y_samples in val_loader:\n",
    "            X_samples = X_samples.to(device)\n",
    "            y_samples = y_samples.to(device)\n",
    "            outputs = model(X_samples)\n",
    "            val_predict += outputs.tolist()\n",
    "            val_target += y_samples.tolist()\n",
    "            loss = criterion(outputs, y_samples)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_r2.append(r_squared(val_target, val_predict))\n",
    "\n",
    "    print(f'\\nEPOCH {epoch + 1}:\\tTraining loss: {train_loss :.3f}\\tValidation loss: {val_loss :.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test R^2: 0.8523630499839783\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat = model(X_test)\n",
    "    test_set_r2 = r_squared(y_hat, y_test)\n",
    "    print(f'\\nTest R^2: {test_set_r2}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxoVns9UMwAn",
        "outputId": "9bb15e3d-484e-434a-8430-0f868dd272db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n"
          ]
        }
      ],
      "source": [
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHMxW0XwMrWB",
        "outputId": "0f733ffb-6eec-457e-a5e0-a2fa30c5f162"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "seed = 1\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "import unidecode\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd52N-vWMrWD"
      },
      "outputs": [],
      "source": [
        "dataset_path = '/content/dataset/all-data.csv'\n",
        "headers = ['sentiment', 'content']\n",
        "df = pd.read_csv(dataset_path, names=headers, encoding='ISO-8859-1')\n",
        "\n",
        "classes = {\n",
        "    class_name: idx for idx, class_name in enumerate(df['sentiment'].unique().tolist())\n",
        "}\n",
        "\n",
        "df['sentiment'] = df['sentiment'].apply(lambda x: classes[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMMn83ncMrWE"
      },
      "outputs": [],
      "source": [
        "english_stop_words = stopwords.words('english')\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def text_normalize(text):\n",
        "    text = text.lower()\n",
        "    text = unidecode.unidecode(text)\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = ' '.join([word for word in text.split(' ') if word not in english_stop_words])\n",
        "    text = ' '.join([stemmer.stem(word) for word in text.split(' ')])\n",
        "\n",
        "    return text\n",
        "\n",
        "df['content'] = df['content'].apply(lambda x: text_normalize(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2q2WQqHMrWE"
      },
      "outputs": [],
      "source": [
        "vocab = []\n",
        "for sentence in df['content'].tolist():\n",
        "    tokens = sentence.split()\n",
        "    for token in tokens:\n",
        "        if token not in vocab:\n",
        "            vocab.append(token)\n",
        "\n",
        "vocab.append('UNK')\n",
        "vocab.append('PAD')\n",
        "word_to_idx = {word:idx for idx, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfYQGUAqMrWE"
      },
      "outputs": [],
      "source": [
        "def transform(text, word_to_idx, max_seq_len):\n",
        "    tokens = []\n",
        "    for w in text.split():\n",
        "        try:\n",
        "            w_ids = word_to_idx[w]\n",
        "        except:\n",
        "            w_ids = word_to_idx['UNK']\n",
        "        tokens.append(w_ids)\n",
        "\n",
        "    if len(tokens) < max_seq_len:\n",
        "        tokens += [word_to_idx['PAD']] * (max_seq_len - len(tokens))\n",
        "    elif len(tokens) > max_seq_len:\n",
        "        tokens = tokens[:max_seq_len]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqrZhkABMrWF"
      },
      "outputs": [],
      "source": [
        "val_size = 0.2\n",
        "test_size = 0.125\n",
        "is_shuffle = True\n",
        "texts =df['content'].tolist()\n",
        "labels = df['sentiment'].tolist()\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    texts, labels,\n",
        "    test_size = val_size,\n",
        "    random_state = seed,\n",
        "    shuffle = is_shuffle\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_train, y_train,\n",
        "    test_size = test_size,\n",
        "    random_state = seed,\n",
        "    shuffle = is_shuffle\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQJvf14-MrWF"
      },
      "outputs": [],
      "source": [
        "class FinancialNews(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        X, y,\n",
        "        word_to_idx,\n",
        "        max_seq_len,\n",
        "        transform=None\n",
        "    ):\n",
        "        self.texts = X\n",
        "        self.labels = y\n",
        "        self.word_to_idx = word_to_idx\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            text = self.transform(\n",
        "                text,\n",
        "                self.word_to_idx,\n",
        "                self.max_seq_len\n",
        "            )\n",
        "\n",
        "        text = torch.tensor(text)\n",
        "\n",
        "        return text, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3a1AtcDNMrWF"
      },
      "outputs": [],
      "source": [
        "max_seq_len = 32\n",
        "\n",
        "train_dataset = FinancialNews(\n",
        "    X_train, y_train,\n",
        "    word_to_idx=word_to_idx,\n",
        "    max_seq_len=max_seq_len,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "val_dataset = FinancialNews(\n",
        "    X_val, y_val,\n",
        "    word_to_idx=word_to_idx,\n",
        "    max_seq_len=max_seq_len,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = FinancialNews(\n",
        "    X_test, y_test,\n",
        "    word_to_idx=word_to_idx,\n",
        "    max_seq_len=max_seq_len,\n",
        "    transform=transform\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWnuMh-3MrWF"
      },
      "outputs": [],
      "source": [
        "train_batch_size = 128\n",
        "test_batch_size = 8\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=train_batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=test_batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=test_batch_size,\n",
        "    shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmnIO4FbMrWG"
      },
      "outputs": [],
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self, vocab_size, embedding_dim,\n",
        "        hidden_size, n_layers, n_classes,\n",
        "        dropout_prob\n",
        "    ):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_size, n_layers, batch_first=True)\n",
        "        self.norm = nn.LayerNorm(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.fc1 = nn.Linear(hidden_size, 16)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(16, n_classes)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.embedding(x)\n",
        "        x, hn = self.rnn(x)\n",
        "        x = x[:, -1, :]\n",
        "        x = self.norm(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZJVT36zMrWG"
      },
      "outputs": [],
      "source": [
        "n_classes = len(list(classes.keys()))\n",
        "embedding_dim = 64\n",
        "hidden_size = 64\n",
        "n_layers = 2\n",
        "dropout_prob = 0.2\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = SentimentClassifier(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_size=hidden_size,\n",
        "    n_layers=n_layers,\n",
        "    n_classes=n_classes,\n",
        "    dropout_prob=dropout_prob\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1OOUuKNMrWG"
      },
      "outputs": [],
      "source": [
        "lr =1e-4\n",
        "epochs = 50\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hclq70SaMrWG"
      },
      "outputs": [],
      "source": [
        "def fit(\n",
        "    model, train_loader, val_loader,\n",
        "    criterion, optimizer, device, epochs\n",
        "):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        batch_train_losses = []\n",
        "        model.train()\n",
        "        for idx, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_train_losses.append(loss.item())\n",
        "\n",
        "        train_loss = sum(batch_train_losses) / len(batch_train_losses)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        val_loss, val_acc = evaluate(\n",
        "            model, val_loader,\n",
        "            criterion, device\n",
        "        )\n",
        "\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f'EPOCH {epoch+1}: \\tTrain loss: {train_loss:.4f} \\tVal loss: {val_loss:.4f}')\n",
        "\n",
        "    return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hidovcP9MrWG"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            losses.append(loss.item())\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    loss = sum(losses) / len(losses)\n",
        "    acc = correct / total\n",
        "\n",
        "    return loss, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CEO7d1NMrWH",
        "outputId": "e7def0e7-e953-4202-e514-d63480450ca2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH 1: \tTrain loss: 1.2541 \tVal loss: 1.2485\n",
            "EPOCH 2: \tTrain loss: 1.2563 \tVal loss: 1.2485\n",
            "EPOCH 3: \tTrain loss: 1.2545 \tVal loss: 1.2485\n",
            "EPOCH 4: \tTrain loss: 1.2536 \tVal loss: 1.2485\n",
            "EPOCH 5: \tTrain loss: 1.2575 \tVal loss: 1.2485\n",
            "EPOCH 6: \tTrain loss: 1.2536 \tVal loss: 1.2485\n",
            "EPOCH 7: \tTrain loss: 1.2542 \tVal loss: 1.2485\n",
            "EPOCH 8: \tTrain loss: 1.2585 \tVal loss: 1.2485\n",
            "EPOCH 9: \tTrain loss: 1.2569 \tVal loss: 1.2485\n",
            "EPOCH 10: \tTrain loss: 1.2558 \tVal loss: 1.2485\n",
            "EPOCH 11: \tTrain loss: 1.2542 \tVal loss: 1.2485\n",
            "EPOCH 12: \tTrain loss: 1.2546 \tVal loss: 1.2485\n",
            "EPOCH 13: \tTrain loss: 1.2579 \tVal loss: 1.2485\n",
            "EPOCH 14: \tTrain loss: 1.2583 \tVal loss: 1.2485\n",
            "EPOCH 15: \tTrain loss: 1.2554 \tVal loss: 1.2485\n",
            "EPOCH 16: \tTrain loss: 1.2536 \tVal loss: 1.2485\n",
            "EPOCH 17: \tTrain loss: 1.2570 \tVal loss: 1.2485\n",
            "EPOCH 18: \tTrain loss: 1.2553 \tVal loss: 1.2485\n",
            "EPOCH 19: \tTrain loss: 1.2555 \tVal loss: 1.2485\n",
            "EPOCH 20: \tTrain loss: 1.2586 \tVal loss: 1.2485\n",
            "EPOCH 21: \tTrain loss: 1.2582 \tVal loss: 1.2485\n",
            "EPOCH 22: \tTrain loss: 1.2565 \tVal loss: 1.2485\n",
            "EPOCH 23: \tTrain loss: 1.2561 \tVal loss: 1.2485\n",
            "EPOCH 24: \tTrain loss: 1.2550 \tVal loss: 1.2485\n",
            "EPOCH 25: \tTrain loss: 1.2571 \tVal loss: 1.2485\n",
            "EPOCH 26: \tTrain loss: 1.2566 \tVal loss: 1.2485\n",
            "EPOCH 27: \tTrain loss: 1.2579 \tVal loss: 1.2485\n",
            "EPOCH 28: \tTrain loss: 1.2592 \tVal loss: 1.2485\n",
            "EPOCH 29: \tTrain loss: 1.2587 \tVal loss: 1.2485\n",
            "EPOCH 30: \tTrain loss: 1.2591 \tVal loss: 1.2485\n",
            "EPOCH 31: \tTrain loss: 1.2542 \tVal loss: 1.2485\n",
            "EPOCH 32: \tTrain loss: 1.2563 \tVal loss: 1.2485\n",
            "EPOCH 33: \tTrain loss: 1.2585 \tVal loss: 1.2485\n",
            "EPOCH 34: \tTrain loss: 1.2564 \tVal loss: 1.2485\n",
            "EPOCH 35: \tTrain loss: 1.2598 \tVal loss: 1.2485\n",
            "EPOCH 36: \tTrain loss: 1.2528 \tVal loss: 1.2485\n",
            "EPOCH 37: \tTrain loss: 1.2560 \tVal loss: 1.2485\n",
            "EPOCH 38: \tTrain loss: 1.2565 \tVal loss: 1.2485\n",
            "EPOCH 39: \tTrain loss: 1.2578 \tVal loss: 1.2485\n",
            "EPOCH 40: \tTrain loss: 1.2540 \tVal loss: 1.2485\n",
            "EPOCH 41: \tTrain loss: 1.2578 \tVal loss: 1.2485\n",
            "EPOCH 42: \tTrain loss: 1.2542 \tVal loss: 1.2485\n",
            "EPOCH 43: \tTrain loss: 1.2587 \tVal loss: 1.2485\n",
            "EPOCH 44: \tTrain loss: 1.2549 \tVal loss: 1.2485\n",
            "EPOCH 45: \tTrain loss: 1.2545 \tVal loss: 1.2485\n",
            "EPOCH 46: \tTrain loss: 1.2563 \tVal loss: 1.2485\n",
            "EPOCH 47: \tTrain loss: 1.2571 \tVal loss: 1.2485\n",
            "EPOCH 48: \tTrain loss: 1.2512 \tVal loss: 1.2485\n",
            "EPOCH 49: \tTrain loss: 1.2552 \tVal loss: 1.2485\n",
            "EPOCH 50: \tTrain loss: 1.2540 \tVal loss: 1.2485\n"
          ]
        }
      ],
      "source": [
        "train_losses, val_losses = fit(\n",
        "    model, train_loader, val_loader,\n",
        "    criterion, optimizer, device, epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DDpcJSQMrWH",
        "outputId": "8eae2727-30df-42a9-f4e4-efeee561dc32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation on val / test dataset\n",
            "Val accuracy:  0.12474226804123711\n",
            "Test accuracy:  0.12164948453608247\n"
          ]
        }
      ],
      "source": [
        "val_loss, val_acc = evaluate(\n",
        "    model, val_loader,\n",
        "    criterion, device\n",
        ")\n",
        "\n",
        "test_loss, test_acc = evaluate(\n",
        "    model, test_loader,\n",
        "    criterion, device\n",
        ")\n",
        "\n",
        "print('Evaluation on val / test dataset')\n",
        "print('Val accuracy: ', val_acc)\n",
        "print('Test accuracy: ', test_acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
